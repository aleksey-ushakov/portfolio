{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a04979e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.python.keras.layers import deserialize, serialize\n",
    "from tensorflow.keras.layers import Input, Embedding, LayerNormalization, concatenate, LSTM, BatchNormalization, Dense, Reshape\n",
    "from tensorflow.python.keras.saving import saving_utils\n",
    "from tensorflow.random import set_seed\n",
    "from tensorflow.keras.optimizers import Nadam\n",
    "from sklearn.utils import shuffle\n",
    "from itertools import product\n",
    "\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import os, sys\n",
    "import datetime\n",
    "import json\n",
    "import dill as pickle #более мощная библиотека позволяющая сохранять функции\n",
    "import random\n",
    "import copy\n",
    "\n",
    "from IPython.display import display\n",
    "\n",
    "from scipy.stats import kstest, anderson #lilliefors\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split, ShuffleSplit, cross_val_score, learning_curve\n",
    "from sklearn.model_selection import KFold, GridSearchCV, RandomizedSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import classification_report, f1_score, precision_score, recall_score, r2_score, roc_auc_score\n",
    "from sklearn.utils import resample\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "\n",
    "import catboost as catb\n",
    "\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "%matplotlib inline\n",
    "pd.set_option('display.max_columns', None)\n",
    "from utils import short_model_score_report, scoring_report, report_by_product, slice_report\n",
    "now_str = lambda : datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c0387f47",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class model_dev:\n",
    "    \"\"\"\n",
    "    Class builds container with predictive models based on parameters\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "        \n",
    "    df: pandas DataFrame\n",
    "        Datafrae with model train data. Must include column with name equal to TARGET_NAME parameter \n",
    "    \n",
    "    TARGET_NAME: string\n",
    "        Name of column in train dataframe with training target (0 or 1 for classification)\n",
    "    \n",
    "    models: dictionary\n",
    "        Python dictionary with models like {'model_title': model_object}. Where 'model_title': string model name; model_object: model object with preset parameters.\n",
    "        Model objects must have folowing methods: fit, predict, predict_proba\n",
    "\n",
    "    cat_columns: list of strings\n",
    "        List of categorial columns name\n",
    "        \n",
    "    scaler_columns: list of strings\n",
    "        List of numeric columns to be scaled\n",
    "        \n",
    "    scaler_type: string\n",
    "        Type of scaler applyed to columns in scaler_columns list. \n",
    "        'std'  - standartization\n",
    "        'norm' - normalization\n",
    "        'none' - no scaling applyed\n",
    "        Default value: 'std'\n",
    "    \n",
    "    target_class_ratio: float\n",
    "        Taining data will be rebalanced to match portion of class '1' in data with target_class_ratio\n",
    "        if target_class_ratio = -1, training data will not be rebalanced\n",
    "        Default value: 0.5\n",
    "        \n",
    "    downsample: bool\n",
    "        If False upsampleing will be applyed during training data rebalancing class '1' to target_class_ratio\n",
    "        If True - downsampling\n",
    "        Default value: False\n",
    "                 \n",
    "    SKF_splits: integer\n",
    "        Number of Stratified K folds splits and consiquently number of models (each SKF split is used for validation while left data is used for model training)\n",
    "        Default value: 5\n",
    "                 \n",
    "    sample_weight: bool\n",
    "        If True wights of each sample in CatboostClassifier set to portion of target it's class in training data\n",
    "        This allows to shift point of class division to 0.5\n",
    "        Default value: True\n",
    "\n",
    "    random_state: integer\n",
    "        random_state fix random seed for model building\n",
    "        Default value: 42\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    # Class for data scailing\n",
    "    class Scaler:\n",
    "        \n",
    "        \"\"\"\n",
    "        Class scales input data\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        \n",
    "        scaler_type: string\n",
    "        Type of scaler applyed to columns in scaler_columns list. \n",
    "        'std'  - standartization\n",
    "        'norm' - normalization\n",
    "        'none' - no scaling applyed\n",
    "        Default value: 'std'\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "\n",
    "        def __init__(self, scaler_type='std'):\n",
    "            self.scaler_type = scaler_type\n",
    "            self.min = 0\n",
    "            self.max = 0\n",
    "            self.std = 0\n",
    "            self.mean = 0\n",
    "\n",
    "        def fit(self, data):\n",
    "            \n",
    "            self.min = np.min(data, axis=0)\n",
    "            self.max = np.max(data, axis=0)\n",
    "            self.std = np.std(data, axis=0)\n",
    "            self.mean = np.mean(data, axis=0)\n",
    "            return self\n",
    "\n",
    "        def train(self, data):\n",
    "            fit(self, data)\n",
    "\n",
    "        def transform (self, data):\n",
    "            if self.scaler_type == 'std':\n",
    "                return self.compact_types((data - self.mean) / self.std)\n",
    "\n",
    "            if self.scaler_type == 'norm':\n",
    "                return self.compact_types((data - self.min) / (self.max-self.min))\n",
    "\n",
    "            if self.scaler_type == 'none':\n",
    "                return data\n",
    "\n",
    "        def fit_transform(self, data):\n",
    "            fit(self, data)\n",
    "            transform (self, data)\n",
    "\n",
    "        def compact_types(self, data):\n",
    "            cols = data.select_dtypes(include='float64').columns\n",
    "            data[cols] = data[cols].astype('float32')\n",
    "            data.replace({-np.inf:np.nan, np.inf:np.nan}, inplace=True)\n",
    "            return data\n",
    "    \n",
    "    def __init__(self,\n",
    "                 df,\n",
    "                 TARGET_NAME,\n",
    "                 models={},\n",
    "                 cat_columns=[],\n",
    "                 scaler_columns=[],\n",
    "                 scaler_type = 'std',\n",
    "                 target_class_ratio=0.5,\n",
    "                 downsample = False,\n",
    "                 SKF_splits=5,\n",
    "                 sample_weight=True,\n",
    "                 random_state=42):\n",
    "        \n",
    "        # Vertion of model\n",
    "        self.version = '1'\n",
    "        \n",
    "        self.df = df\n",
    "        self.TARGET_NAME = TARGET_NAME\n",
    "        self.df_bl_index = None\n",
    "        self.df_bl_pr = None\n",
    "        \n",
    "        self.df_columns = list(df.columns)\n",
    "        self.model_columns = None\n",
    "        self.cat_columns = cat_columns\n",
    "        \n",
    "        self.scaler_columns = scaler_columns\n",
    "        self.scaler_type = scaler_type\n",
    "\n",
    "        self.target_class_ratio = target_class_ratio\n",
    "        self.downsample = downsample\n",
    "\n",
    "        self.SKF_splits = SKF_splits\n",
    "        self.SKF_list = None\n",
    "\n",
    "        self.sample_weight = sample_weight\n",
    "\n",
    "        self.random_state = random_state\n",
    "\n",
    "\n",
    "        self.models = []\n",
    "        \n",
    "        for model in models.keys():\n",
    "            self.models.append({'model_name':model,\n",
    "                                'model_class':models[model].__class__.__name__,\n",
    "                                'model_sample': copy.deepcopy(models[model]),\n",
    "                                'feature_importance':None,\n",
    "                                'folds':[{'fold':i+1,\n",
    "                                          'scaler':None,\n",
    "                                          'trained_model':None,\n",
    "                                          'feature_prediction_values_change':None,                                                \n",
    "                                          'feature_loss_function_change':None,\n",
    "                                          'feature_interaction_importance':None} for i in range(SKF_splits)]})\n",
    "\n",
    "\n",
    "    # Models training\n",
    "    def fit(self, fold_report=True, train_report=False):\n",
    "        \n",
    "        \"\"\"\n",
    "        Train models\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        \n",
    "        fold_report: bool\n",
    "            If true each fold metrics will be shown         \n",
    "            Default value: True\n",
    "        \n",
    "        train_report: bool\n",
    "            If true train data metrics will be shown \n",
    "            Default value: False   \n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        now_str = lambda : datetime.datetime.now().strftime(\"%H:%M:%S\")        \n",
    "\n",
    "        # Making dataset for train results recording\n",
    "        self.train_result = self.df[[self.TARGET_NAME]]\n",
    "        for model in self.models:\n",
    "            self.train_result[model['model_name']] = float(\"NaN\")\n",
    "            self.train_result[model['model_name'] + '_p'] =  float(\"NaN\")\n",
    "\n",
    "        # Balancing training data by target class (works only for classification models)\n",
    "        self._balance_df_by_target()\n",
    "        \n",
    "        # Data preprocessing\n",
    "        self.df_bl_pr = self._data_processing(self.df.loc[self.df_bl_index], training_mode=True)\n",
    "        \n",
    "        # Fixing model columns list\n",
    "        self.model_columns = list(self.df_bl_pr.drop(columns = self.TARGET_NAME))\n",
    "        self.cat_columns = list(set(self.cat_columns) & set(self.model_columns))\n",
    "        self.scaler_columns = list(set(self.scaler_columns) & set(self.model_columns))\n",
    "        \n",
    "        # Forming SKF indexes for crossvalidation\n",
    "        self._SKF(df=self.df_bl_pr)\n",
    "        \n",
    "        split = 0\n",
    "        for train_index, valid_index in self.SKF_list:\n",
    "            split += 1\n",
    "            \n",
    "            # Training and validation dataframes creation\n",
    "            df_train_pr = self.df_bl_pr.loc[train_index].copy()\n",
    "            df_valid_pr = self.df_bl_pr.loc[valid_index].copy()\n",
    "            \n",
    "            # Scaling data (critical for linear models)\n",
    "            df_train_pr_sc = df_train_pr\n",
    "            df_valid_pr_sc = df_valid_pr\n",
    "            \n",
    "            data_scaler = self.Scaler(scaler_type=self.scaler_type).fit(df_train_pr[self.scaler_columns])\n",
    "\n",
    "            df_train_pr_sc[self.scaler_columns] = data_scaler.transform(df_train_pr[self.scaler_columns])\n",
    "            df_valid_pr_sc[self.scaler_columns] = data_scaler.transform(df_valid_pr[self.scaler_columns])\n",
    "            \n",
    "            X_train = df_train_pr_sc.drop(columns=self.TARGET_NAME)\n",
    "            y_train = df_train_pr_sc[self.TARGET_NAME]\n",
    "\n",
    "            X_valid = df_valid_pr_sc.drop(columns=self.TARGET_NAME)\n",
    "            y_valid = df_valid_pr_sc[self.TARGET_NAME]\n",
    "            \n",
    "            # removing unnecessary objects\n",
    "            del(df_train_pr)\n",
    "            del(df_train_pr_sc)\n",
    "            del(df_valid_pr)\n",
    "            del(df_valid_pr_sc)\n",
    "\n",
    "            cat_feat_idx = list(np.where(X_train.columns.isin(self.cat_columns))[0])\n",
    "            \n",
    "            # Weights for each sample\n",
    "            if self.sample_weight:\n",
    "                class_weight_dict = dict(1 / (y_train.value_counts()/y_train.shape[0]))\n",
    "                train_sample_weight = y_train.map(class_weight_dict)\n",
    "            else:\n",
    "                sample_weight = np.ones(y_train.shape[0])\n",
    "            \n",
    "            if len([1 for model in self.models if model['model_class'] in ['CatBoostClassifier', 'CatBoostRegressor']]) > 0:\n",
    "                catb_valid_pool = catb.Pool(data=X_valid, label=y_valid, cat_features=cat_feat_idx)\n",
    "                catb_train_pool = catb.Pool(data=X_train, label=y_train, cat_features=cat_feat_idx, weight=train_sample_weight)\n",
    "            \n",
    "            # Training models\n",
    "            for model in self.models:\n",
    "\n",
    "                model_name = model['model_name']\n",
    "                model_to_train = copy.deepcopy(model['model_sample'])\n",
    "                \n",
    "                if model['model_class'] == 'CatBoostClassifier':\n",
    "                    model_to_train.fit(X=catb_train_pool, eval_set=catb_valid_pool, plot=False, use_best_model=True)\n",
    "                    # saving features importance data on validation datasets\n",
    "                    model['folds'][split-1]['feature_prediction_values_change'] = model_to_train.get_feature_importance(data=catb_valid_pool)\n",
    "                    model['folds'][split-1]['feature_loss_function_change'] = model_to_train.get_feature_importance(type='LossFunctionChange', data=catb_valid_pool)\n",
    "                    model['folds'][split-1]['feature_interaction_importance'] = model_to_train.get_feature_importance(type='Interaction', data=catb_valid_pool)\n",
    "\n",
    "                elif model['model_class'] == 'CatBoostRegressor':            \n",
    "                    model_to_train.fit(X=catb_train_pool, eval_set=(X_valid, y_valid), plot=False, use_best_model=True)\n",
    "\n",
    "                else:\n",
    "                    #X_train.fillna(0, inplace=True)\n",
    "                    #X_valid.fillna(0, inplace=True)\n",
    "                    model_to_train.fit(X_train, y_train, sample_weight=train_sample_weight)\n",
    "\n",
    "                # Saving trained model for each fold\n",
    "                model['folds'][split-1]['scaler'] = copy.deepcopy(data_scaler)\n",
    "                model['folds'][split-1]['trained_model'] = copy.deepcopy(model_to_train)\n",
    "                \n",
    "                # wrighting model predictions on validation dataset\n",
    "                y_valid_pred_proba = model_to_train.predict_proba(X_valid)[:,1]\n",
    "                y_valid_pred = np.round(y_valid_pred_proba).astype(int)\n",
    "                self.train_result.loc[valid_index, model_name + '_p'] = y_valid_pred_proba\n",
    "                self.train_result.loc[valid_index, model_name] = y_valid_pred\n",
    "\n",
    "                # Making report for each fold\n",
    "                if fold_report:\n",
    "                    if (model_name == self.models[0]['model_name']):\n",
    "                        print('\\nFOLD ' + str(split) + ' REPORT')\n",
    "\n",
    "                    short_model_score_report(y_true=y_valid, \n",
    "                                             y_pred_proba=y_valid_pred_proba, \n",
    "                                             name=model_name + '_' +str(split) + '_valid', \n",
    "                                             header=(model_name == self.models[0]['model_name']),\n",
    "                                             model_type='classification')\n",
    "\n",
    "                    #  Making report for training data\n",
    "                    if train_report:\n",
    "                        short_model_score_report(y_true=y_train, \n",
    "                                                 y_pred_proba=model_to_train.predict_proba(X_train)[:,1], \n",
    "                                                 name=len(model_name + '_' +str(split)) * ' ' + ' train',\n",
    "                                                 header=False,\n",
    "                                                 model_type='classification')\n",
    "\n",
    "        \n",
    "        # Making final report (all folds))\n",
    "        print('\\nFINAL REPORT ({} folds AVG)'.format(self.SKF_splits))\n",
    "        slice_index = self.df_bl_index\n",
    "        \n",
    "        # Result of each model on all folds\n",
    "        for model in self.models:\n",
    "\n",
    "            short_model_score_report(y_true=self.train_result[self.TARGET_NAME].loc[slice_index],\n",
    "                                     y_pred_proba=self.train_result[model['model_name']+'_p'].loc[slice_index],\n",
    "                                     name=model['model_name'],\n",
    "                                     header=(model['model_name'] == self.models[0]['model_name']),\n",
    "                                     model_type='classification')\n",
    "        \n",
    "        # Average result of all models\n",
    "        y_valid = self.df[self.TARGET_NAME].loc[slice_index]\n",
    "        y_valid_pred_proba = np.sum(self.train_result[[model['model_name'] + '_p' for model in self.models]],axis=1) / len(self.models)\n",
    "        y_valid_pred_proba = y_valid_pred_proba.loc[slice_index]\n",
    "        y_valid_pred = np.round(y_valid_pred_proba).astype(int)\n",
    "\n",
    "        short_model_score_report(y_true=y_valid,\n",
    "                                 y_pred_proba=y_valid_pred_proba,\n",
    "                                 name='MIX RESULT ',\n",
    "                                 header=False,\n",
    "                                 model_type='classification')\n",
    "\n",
    "        # Calculation of final scores and probabilities for all models\n",
    "        p_cols = [col for col in self.train_result.columns if col[-2:]=='_p']\n",
    "        self.train_result['proba'] = np.sum(self.train_result[p_cols], axis=1)\n",
    "        self.train_result['score'] = 1000 - np.sum(self.train_result[p_cols], axis=1) * 1000\n",
    "\n",
    "        # Adding calculations for records which where not taken for model training after balancing\n",
    "        # unscored_result = self.predict(input_type='df', input_data=self.df[~self.df.index.isin(self.df_bl_index)], output_type='df')\n",
    "        # self.train_result.loc[unscored_result.index, unscored_result.columns] = unscored_result\n",
    "        \n",
    "        # Calculation of average feature importance\n",
    "        for model in self.models:\n",
    "            if model['model_class'] == 'CatBoostClassifier':\n",
    "                model['feature_importance'] = self._catb_feature_importance(model)\n",
    "\n",
    "        \n",
    "        # Cleaning memory\n",
    "        self.df = None\n",
    "        self.df_bl_pr = None\n",
    "\n",
    "        \n",
    "    # Data preprocessing\n",
    "    def _data_processing(self, df, training_mode=False):\n",
    "\n",
    "        return df\n",
    "\n",
    "    \n",
    "    # Prediction making method\n",
    "    def predict(self,\n",
    "                input_data=None,\n",
    "                input_type='df',\n",
    "                output_type='df',        # 'df', 'json', 'csv'\n",
    "                output_file_name=None,   # название файла в случае если output_type=='csv'\n",
    "                prod_mode=True,          # If true returns only score (without probability by models)\n",
    "                features=False,          # в выходной массив добавляются занчения признаков\n",
    "                shap_values=False):      # в выходной массив добавлюются shap значения с суффиксом _SHAP\n",
    "        \n",
    "        \n",
    "        \"\"\"\n",
    "        Generating predictions\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        \n",
    "        input_data: pandas DataFrame or string\n",
    "            Input data which includes features for model to make predictions. Can be different format types based on 'input_type' parameter\n",
    "            \n",
    "        input_type: string\n",
    "            Description of input data type, can be:\n",
    "            'df'        - input_data is pandas DataFrame object\n",
    "            'csv'       - input_data is CSV filename\n",
    "            'json'      - input data is JSON string\n",
    "            'json_file' - input data is JSON file (for test purpose only, inefficient because parse file as stdin)\n",
    "            Default value: 'df'\n",
    "        \n",
    "        output_type: string\n",
    "            Format of method return:\n",
    "            'df'        - returns pandas DataFrame object\n",
    "            'json'      - returns JSON string with predictions\n",
    "            'csv'       - returns CSV file with predictions\n",
    "            Default value: 'df'        \n",
    "        \n",
    "        output_file_name: string\n",
    "            Filename for predictions CSV file in case if output_type is set to 'csv'\n",
    "            Default value: None\n",
    "            \n",
    "        prod_mode: bool\n",
    "            Format of predictions generated. \n",
    "            If True, method returns only 'score' for each input record calclated as (1 - probability) * 1000\n",
    "            If False, method in addition to 'score' returns target class probability for each model and resulted everage probability\n",
    "            \n",
    "        features: bool\n",
    "            If True, method returns model features in addition to predictions\n",
    "            \n",
    "        shap_values: bool\n",
    "            If True, method returns shap values for each feature in addition to predictions\n",
    "            \n",
    "        \"\"\"                         \n",
    "        \n",
    "        # Формирование предсказания из входного датафрейма и контейнера с моделями\n",
    "        def predict_from_df(df_test, shap_values=False, features=False):\n",
    "\n",
    "            model_name_lst = [model['model_name'] for model in self.models]\n",
    "\n",
    "            # Формирование датасета для записи результатом работы моделей\n",
    "            df_test_result = pd.DataFrame(index=df_test.index)\n",
    "            \n",
    "            #Обработка данных функцией и выстраивание одинаковой очередности и кол-ва признаков                \n",
    "            df_test_pr = self._data_processing(df_test.copy(), training_mode=False)[self.model_columns]\n",
    "            \n",
    "            for model in self.models:\n",
    "                # Формирование дополнительныъ колонок для записи результатом работы моделей\n",
    "                df_test_result[model['model_name']] = np.zeros(df_test.shape[0])\n",
    "                df_test_result[model['model_name'] + '_p'] = np.zeros(df_test.shape[0])    \n",
    "                    \n",
    "                for fold in model['folds']:\n",
    "                    \n",
    "                    data_scaler = fold['scaler']\n",
    "                    tr_model = fold['trained_model']\n",
    "\n",
    "                    # Масштабирование данных для моделей (критично для линейных моделей)\n",
    "                    df_test_pr_sc = df_test_pr.copy()\n",
    "                    df_test_pr_sc[self.scaler_columns] = data_scaler.transform(df_test_pr[self.scaler_columns])\n",
    "                    X_test = df_test_pr_sc\n",
    "                    \n",
    "                    # Обработка пропусков\n",
    "                    if model['model_class'] != 'CatBoostClassifier':\n",
    "                        X_test.replace({-np.inf:0, np.inf:0, np.nan:0}, inplace=True)\n",
    "\n",
    "                    # Запись предсказаний моделей в журнал\n",
    "                    y_pred_proba = tr_model.predict_proba(X_test)[:,1]\n",
    "\n",
    "                    if shap_values:\n",
    "                        cat_feat_idx = list(np.where(X_test.columns.isin(self.cat_columns))[0])\n",
    "                        catb_valid_pool=catb.Pool(data=X_test, label=None, cat_features=cat_feat_idx)\n",
    "                        if fold['fold'] == 1:\n",
    "                            shap_array = tr_model.get_feature_importance(type='ShapValues', data=catb_valid_pool)\n",
    "                        else:\n",
    "                            shap_array += tr_model.get_feature_importance(type='ShapValues', data=catb_valid_pool)\n",
    "\n",
    "                    df_test_result[model['model_name'] + '_p'] += y_pred_proba\n",
    "                    df_test_result[model['model_name']] += np.round(y_pred_proba)\n",
    "\n",
    "                \n",
    "                df_test_result = df_test_result / len(model['folds'])                    \n",
    "                    \n",
    "                if shap_values and (model['model_class'] == 'CatBoostClassifier'):\n",
    "                    shap_array = shap_array / len(model['folds'])\n",
    "                    shap_cols = [model['model_name'] + '_shap_' + col for col in self.model_columns] + [model['model_name'] + '_shap_BASE_VALUE']\n",
    "                    shap_df =  pd.DataFrame(data=shap_array, columns=shap_cols, index=df_test_result.index)\n",
    "                    shap_df[model['model_name'] + '_shap_RESULT_VALUE'] = np.sum(shap_df.iloc[:,:-1], axis=1)\n",
    "                    df_test_result = pd.concat((df_test_result, shap_df), axis=1)\n",
    "                          \n",
    "            # Округление до 0 знаков среднего значения предскзаний модели на всех фолдах \n",
    "            df_test_result[model_name_lst] =  np.round(df_test_result[model_name_lst]).astype(int)\n",
    "            p_cols = [col for col in df_test_result.columns if col[-2:]=='_p']\n",
    "            df_test_result['proba'] = np.sum(df_test_result[p_cols], axis=1)\n",
    "            df_test_result['score'] = np.round(1000 - np.sum(df_test_result[p_cols], axis=1) * 1000).astype(int)\n",
    "\n",
    "            if prod_mode:\n",
    "                drop_cols=['proba'] + model_name_lst + p_cols\n",
    "                df_test_result.drop(columns = drop_cols, inplace=True)\n",
    "            \n",
    "            if features:\n",
    "                df_test_result = pd.concat((df_test[self.model_columns],\n",
    "                                            df_test_result),\n",
    "                                           axis=1)\n",
    "                \n",
    "            return df_test_result\n",
    "\n",
    "        \n",
    "        # JSON string as input\n",
    "        def predict_from_json(df_json, shap_values=False, features=False):\n",
    "            \n",
    "            # Converting JSON to pandas DataFrame\n",
    "            df = pd.DataFrame(json.loads(input_data)['data'])\n",
    "            \n",
    "            # Making predictions\n",
    "            df_pred = predict_from_df(df.set_index('dataItemId'), shap_values=shap_values, features=features)\n",
    "\n",
    "            return df_pred\n",
    "        \n",
    "        \n",
    "        # JSON file as STDIN input (redundunt)\n",
    "        def predict_from_json_file(file_name, shap_values=False, features=False):\n",
    "            read_stdin = ''\n",
    "            open_counter = 0\n",
    "            json_counter = 0\n",
    "            pred =pd.DataFrame()\n",
    "\n",
    "            with open(file_name, 'r') as stdandar_input:\n",
    "                for line in stdandar_input:\n",
    "                    for s in line:\n",
    "                        open_counter += (s == '{')\n",
    "                        open_counter -= (s == '}')\n",
    "                        read_stdin += s\n",
    "                        if open_counter == 0:\n",
    "                            if read_stdin[0] == '{':\n",
    "                                json_counter += 1\n",
    "                                if pred.shape[0] == 0:\n",
    "                                    pred = predict_from_json(df_json=read_stdin, shap_values=shap_values, features=features)\n",
    "                                else:\n",
    "                                    pred = pd.concat([pred, predict_from_json(df_json=read_stdin, shap_values=shap_values, features=features)], axis=0)\n",
    "                            read_stdin = ''\n",
    "            return pred\n",
    "\n",
    "        # Input data processing\n",
    "        if input_type == 'df':\n",
    "            df_result = predict_from_df(input_data, shap_values=shap_values, features=features)\n",
    "\n",
    "        elif input_type == 'csv':\n",
    "            with open(input_data, encoding='utf8') as f:\n",
    "                h = f.readline()\n",
    "            if h.count(';')>0: \n",
    "                df_result = predict_from_df(pd.read_csv(input_data,';'), shap_values=shap_values, features=features)\n",
    "            else:\n",
    "                df_result = predict_from_df(pd.read_csv(input_data,','), shap_values=shap_values, features=features)\n",
    "            \n",
    "        elif input_type == 'json':\n",
    "            df_result = predict_from_json(input_data, shap_values=shap_values, features=features)\n",
    "\n",
    "        elif input_type == 'json_file':\n",
    "            df_result = predict_from_json_file(input_data, shap_values=shap_values, features=features)\n",
    "\n",
    "        # returning predictions\n",
    "        if output_type == 'df':\n",
    "            return df_result\n",
    "\n",
    "        if output_type == 'json':                                     \n",
    "\n",
    "            # converting numpy data types to JSON compatible data types function\n",
    "            def convert_to_json_type(x):\n",
    "                if str(type(x))[:-4] == \"<class 'numpy.int\":\n",
    "                    x = int(x)\n",
    "                elif str(type(x))[:-4] == \"<class 'numpy.float'>\":\n",
    "                    x = float(x)\n",
    "                else:\n",
    "                    x = str(x) \n",
    "                return x\n",
    "\n",
    "            df_result.reset_index(inplace=True)\n",
    "            cols = list(df_result.columns)\n",
    "\n",
    "            pred_dict = {'version': self.version, \n",
    "                         'data': [{col: convert_to_json_type(df_result[col].values[i]) for col in cols} for i in range(df_result.shape[0])]}\n",
    "\n",
    "            return json.dumps(pred_dict)    \n",
    "\n",
    "        if output_type == 'csv':\n",
    "            df_result.to_csv(output_file_name)\n",
    "            return 'Data exported to file: ' + output_file_name\n",
    "\n",
    "    # split data into stratified folds\n",
    "    def _SKF(self, df):\n",
    "        TARGET_NAME = self.TARGET_NAME\n",
    "        n_splits = self.SKF_splits\n",
    "        max_layers_qty = 2\n",
    "        random_state = self.random_state\n",
    "        \n",
    "        if n_splits==1:\n",
    "            return [[np.array(df.index), np.array(df.index)]]\n",
    "\n",
    "        # Determine qty of layers\n",
    "        layers_qty = min(max_layers_qty, np.unique(df[TARGET_NAME]).shape[0])\n",
    "        # calculating split points\n",
    "        split_points = np.linspace(np.min(df[TARGET_NAME]), np.max(df[TARGET_NAME]), layers_qty+1)\n",
    "        #split_points = np.quantile(np.sort(np.unique(df[TARGET_NAME])), np.linspace(0, 1, layers_qty+1))\n",
    "\n",
    "        # forming list with indexes and list with training and validation folds\n",
    "        layers=[]\n",
    "        SKF_list = [[np.array([]),np.array([])] for i in range(n_splits)]\n",
    "\n",
    "        np.random.seed(random_state) \n",
    "\n",
    "        for i in range(len(split_points)-1):\n",
    "            # determining indexes to be incuded in layer\n",
    "            if i == len(split_points)-2:\n",
    "                layer_index = np.array(df.loc[(df[TARGET_NAME]>=split_points[i]) & (df[TARGET_NAME]<=split_points[i+1])].index)\n",
    "            else:\n",
    "                layer_index = np.array(df.loc[(df[TARGET_NAME]>=split_points[i]) & (df[TARGET_NAME]< split_points[i+1])].index)\n",
    "\n",
    "            np.random.shuffle(layer_index)\n",
    "            b_qty = layer_index.shape[0] // n_splits\n",
    "\n",
    "            for j in range(n_splits):\n",
    "                if j == n_splits - 1:              \n",
    "                    SKF_list[j][1] = np.hstack((SKF_list[j][1], layer_index[b_qty * j:]))\n",
    "                    SKF_list[j][0] = np.hstack((SKF_list[j][0], layer_index[0:b_qty * j]))\n",
    "                else:\n",
    "                    SKF_list[j][1] = np.hstack((SKF_list[j][1], layer_index[b_qty * j: b_qty * (j+1)]))\n",
    "                    SKF_list[j][0] = np.hstack((SKF_list[j][0], layer_index[0: b_qty * j], layer_index[b_qty * (j+1):]))\n",
    "        \n",
    "        self.SKF_list = SKF_list\n",
    "    \n",
    "    \n",
    "    # Balancing by taget class\n",
    "    def _balance_df_by_target(self):\n",
    "        \n",
    "        df_bl = self.df\n",
    "        \n",
    "        if self.target_class_ratio != -1:\n",
    "            \n",
    "            counts_0 = (df_bl[self.TARGET_NAME] == 0).sum()\n",
    "            counts_1 = (df_bl[self.TARGET_NAME] == 1).sum()\n",
    "\n",
    "            if self.downsample:\n",
    "                if self.target_class_ratio > (counts_1 / (df_bl.shape[0])):\n",
    "                    keep_to_balance = int(counts_0 - df_bl.shape[0] + counts_1 / self.target_class_ratio)\n",
    "                    cut_class = 0\n",
    "                else:\n",
    "                    keep_to_balance = int(counts_1 - df_bl.shape[0] + counts_0 /(1 - self.target_class_ratio))\n",
    "                    cut_class = 1\n",
    "\n",
    "                sample = df_bl[df_bl[self.TARGET_NAME] == cut_class].sample(n=keep_to_balance, replace=False, random_state=self.random_state)\n",
    "                df_bl = pd.concat([sample, df_bl[df_bl[self.TARGET_NAME] != cut_class]], axis=0)\n",
    "            \n",
    "            else:\n",
    "                if self.target_class_ratio > (counts_1 / (df_bl.shape[0])):\n",
    "                    disbalance = int(counts_0 / (1 - self.target_class_ratio) - df_bl.shape[0])\n",
    "                    add_class = 1\n",
    "                else:\n",
    "                    disbalance = int(counts_1 / self.target_class_ratio - df_bl.shape[0])\n",
    "                    add_class = 0\n",
    "\n",
    "                sample = df_bl[df_bl[self.TARGET_NAME] == add_class].sample(n=disbalance, replace=True, random_state=self.random_state)\n",
    "                df_bl = pd.concat([sample, df_bl], axis=0)\n",
    "\n",
    "                df_bl = df_bl.astype(dtype=dict(self.df.dtypes))\n",
    "        \n",
    "        self.df_bl_index = df_bl.index\n",
    "\n",
    "    \n",
    "    # Feature importance for catboostclassifier models\n",
    "    def _catb_feature_importance(self, catb_trained_model):\n",
    "        \n",
    "        SKF_number = len(catb_trained_model['folds'])\n",
    "        cols = self.model_columns\n",
    "        n_cols = len(cols)\n",
    "\n",
    "        col_dict = {i: self.model_columns[i] for i in range(n_cols)}\n",
    "\n",
    "        feat_importance = pd.DataFrame({'PredictionValuesChange':np.zeros(n_cols),\n",
    "                                        'FeatureLossFunctionChange':np.zeros(n_cols)}, index=cols)\n",
    "\n",
    "        for fold in catb_trained_model['folds']:\n",
    "            feat_importance['PredictionValuesChange'] += pd.DataFrame({'feture_importance': fold['trained_model'].get_feature_importance()}, index=self.model_columns)['feture_importance'] / SKF_number\n",
    "            feat_importance['FeatureLossFunctionChange'] += pd.DataFrame({'feture_importance': fold['feature_loss_function_change']}, index=self.model_columns)['feture_importance'] / SKF_number\n",
    "    \n",
    "        if catb_trained_model['folds'][0]['trained_model'].get_all_params()['depth'] == 1:\n",
    "            feat_importance['feature_interaction'] = np.zeros(feat_importance.shape[0])\n",
    "            return feat_importance.sort_values(by='PredictionValuesChange', ascending=False)\n",
    "        \n",
    "        # making pare wise feature imortance table\n",
    "        fi_interaction = pd.DataFrame(data=catb_trained_model['folds'][0]['feature_interaction_importance'], columns=['f1', 'f2', 'strength'])\n",
    "        fi_interaction['feature_1'] = fi_interaction['f1'].map(col_dict)\n",
    "        fi_interaction['feature_2'] = fi_interaction['f2'].map(col_dict)\n",
    "        fi_interaction['strength_cum'] = np.cumsum(fi_interaction['strength'])\n",
    "        fi_interaction['n_unique'] = None\n",
    "        fi_interaction['unique_features'] = None\n",
    "        for i in range(fi_interaction.shape[0]):\n",
    "            fi_interaction.loc[i, 'n_unique'] = np.unique(np.vstack((fi_interaction.iloc[0:i]['f1'].values, fi_interaction.iloc[0:i]['f2'].values))).shape[0]\n",
    "            unique_fetures = np.unique(np.vstack((fi_interaction.iloc[0:i]['f1'].values, fi_interaction.iloc[0:i]['f2'].values))).astype('int')\n",
    "            fi_interaction.loc[i, 'unique_features'] = str(list(map(col_dict.get, unique_fetures)))[1:-1]\n",
    "\n",
    "        fi_interaction=fi_interaction[['f1','feature_1', 'f2', 'feature_2', 'strength', 'strength_cum', 'n_unique', 'unique_features']]\n",
    "\n",
    "        feat_importance['feature_interaction'] = np.zeros(feat_importance.shape[0])\n",
    "\n",
    "        for i in feat_importance.index:\n",
    "            feat_importance.loc[i, 'feature_interaction'] = fi_interaction.loc[fi_interaction['feature_1']==i, 'strength'].sum() + fi_interaction.loc[fi_interaction['feature_2']==i, 'strength'].sum()\n",
    "        feat_importance['feature_interaction'] =  feat_importance['feature_interaction']  / 2\n",
    "\n",
    "        return feat_importance.sort_values(by='PredictionValuesChange', ascending=False)\n",
    "    \n",
    "\n",
    "    # Plot feature importance for catboostclassifier models\n",
    "    def plot_catb_feature_importance(self, catb_feature_importance_df, top_n=20):\n",
    "        \n",
    "        \"\"\"\n",
    "        Plot feature importance for catboostclassifier models\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        \n",
    "        catb_feature_importance_df: pandas DataFrame\n",
    "            Dataframe with catboost feature importance data generated while model training (i-model feature importance: model.models[i]['feature_importance'])\n",
    "            \n",
    "        top_n: integer\n",
    "            Number of top features to be ploted\n",
    "            Default value: 20\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        feat_importance_short = catb_feature_importance_df.iloc[:top_n,:]\n",
    "        fig, ax = plt.subplots(nrows=1, ncols=3)\n",
    "        fig.set_size_inches (18 , top_n * 0.5)\n",
    "        plt.subplots_adjust(wspace=0.0, hspace=0.0)\n",
    "        ax = ax.flatten()\n",
    "\n",
    "        sns.barplot(feat_importance_short['PredictionValuesChange'], feat_importance_short.index, ax=ax[0])\n",
    "        ax[0].set_title('Feature importance\\n*Prediction Values Change*')\n",
    "        ax[0].set_xlabel('Feature importance')\n",
    "        ax[0].set_ylabel('Features')\n",
    "\n",
    "        sns.barplot(feat_importance_short['feature_interaction'], feat_importance_short.index, ax=ax[1])\n",
    "        ax[1].set_title('Feature importance\\n** Feature interaction')\n",
    "        ax[1].set_xlabel('Feature importance')\n",
    "        ax[1].set_yticks([])\n",
    "\n",
    "        sns.barplot(x=feat_importance_short['FeatureLossFunctionChange'], y= feat_importance_short.index, ax=ax[2])\n",
    "        ax[2].set_title('Feature importance\\n*** FeatureLoss Function Change')\n",
    "        ax[2].set_xlabel('Feature importance')\n",
    "        ax[2].set_ylabel('Features')\n",
    "        ax[2].set_yticks([])\n",
    "\n",
    "        for ax_i in ax:\n",
    "            ax_i.spines['top'].set_visible(False)\n",
    "            ax_i.spines['right'].set_visible(False)\n",
    "            ax_i.spines['bottom'].set_visible(False)\n",
    "            ax_i.spines['left'].set_visible(False)\n",
    "        plt.show();\n",
    "\n",
    "        print('* For each feature, PredictionValuesChange shows how much on average the prediction changes \\\n",
    "    if the feature value changes. The bigger the value of the importance the bigger on average is the \\\n",
    "    change to the prediction value, if this feature is changed')\n",
    "        print('\\nFeature importance values are normalized so that the sum of importances of all features is equal \\\n",
    "    to 100. This is possible because the values of these importances are always non-negative.')\n",
    "        print('\\nFormula values inside different groups may vary significantly in ranking modes. \\\n",
    "    This might lead to high importance values for some groupwise features, even though these \\\n",
    "    features dont have a large impact on the resulting metric value.')\n",
    "        print('\\n** TTL Sum for feature interaction is 100. Calculated as sum of importance of all paired combination with given factor devided by 2')\n",
    "        print('\\n*** For each feature the value represents the difference between the loss value of the model with this \\\n",
    "    feature and without it. The model without this feature is equivalent to the one that would have been trained \\\n",
    "    if this feature was excluded from the dataset.')\n",
    "        print('\\nThis feature importance approximates the difference between metric values calculated on the following models: \\\n",
    "    \\n - The model with the -th feature excluded \\\n",
    "    \\n - The original model with all features')\n",
    "    \n",
    "   \n",
    "    # Saving model container\n",
    "    def save(self, file_name, prod_mode=False):\n",
    "        \n",
    "        save_model = copy.deepcopy(self)        \n",
    "        \n",
    "        if prod_mode:\n",
    "            save_model.SKF_list = None\n",
    "            save_model.df_bl_index = None\n",
    "            save_model.df_bl_pr = None\n",
    "            save_model.train_result = None\n",
    "        \n",
    "            for m in save_model.models:\n",
    "                m['feature_importance'] = None\n",
    "                for fold in m['folds']:\n",
    "                    fold['feature_prediction_values_change'] = None\n",
    "                    fold['feature_loss_function_change'] = None\n",
    "                    fold['feature_interaction_importance'] = None\n",
    "\n",
    "        with open(file_name, 'wb') as f:\n",
    "            pickle.dump(save_model, f)    \n",
    "        del(save_model)\n",
    "        print('Model saved to file:', file_name)\n",
    "    \n",
    "    # Exporting production script\n",
    "    def generate_prod_script(self):\n",
    "        script_text = \"\"\n",
    "        script_text += r\"#!/usr/bin/env python\" + '\\n'\n",
    "        script_text += r\"# coding: utf-8\" + '\\n'\n",
    "        script_text += r\"# noinspection PyUnresolvedReferences\" + '\\n'\n",
    "        script_text += r\"import sys, json, os, dill as pickle, pandas as pd, numpy as np\" + '\\n'\n",
    "        script_text += r\"import warnings\" + '\\n'        \n",
    "        script_text += r\"warnings.simplefilter('ignore')\" + '\\n'\n",
    "        script_text += r\"\" + '\\n'\n",
    "        script_text += r\"model_dev = None\" + '\\n'\n",
    "        script_text += r\"class_name = '\" + self.__class__.__name__[:-4] + \"'\" + '\\n'\n",
    "        script_text += r\"\" + '\\n'\n",
    "        script_text += r\"class \" + self.__class__.__name__[:-4] + \"():\" + '\\n'\n",
    "        script_text += r\"\" + '\\n'\n",
    "        script_text += r\"    def __init__(self):\" + '\\n'\n",
    "        script_text += r\"\" + '\\n'\n",
    "        script_text += r\"        # Загрузка модели\" + '\\n'\n",
    "        script_text += r\"        global model_dev\" + '\\n'\n",
    "        script_text += r\"        if not model_dev:\" + '\\n'\n",
    "        script_text += r\"            model_file_name = 'models' + os.sep + self.__class__.__name__\" + '\\n'\n",
    "        script_text += r\"            model_dev = pickle.load(open(model_file_name, 'rb'))\" + '\\n'\n",
    "        script_text += r\"\" + '\\n'\n",
    "        script_text += r\"        self.model_dev = model_dev\" + '\\n'\n",
    "        script_text += r\"\" + '\\n'\n",
    "        script_text += r\"    def predict(self, input_data=None, input_type='json', output_type='json', output_file_name=None, features=False, shap_values=False, prod_mode=True):\" + '\\n'\n",
    "        script_text += r\"\" + '\\n'\n",
    "        script_text += r\"        pred = self.model_dev.predict(input_type=input_type,              # 'df', 'csv', 'json', 'json_file'\" + '\\n'\n",
    "        script_text += r\"                                      input_data=input_data,              # объект 'df',название файла 'csv', Строка 'json', файл с 'json' строками \" + '\\n'\n",
    "        script_text += r\"                                      output_type=output_type,            # 'df', 'json', 'csv'\" + '\\n'\n",
    "        script_text += r\"                                      output_file_name=output_file_name,  # название файла в случае если output_type=='csv'\" + '\\n'\n",
    "        script_text += r\"                                      prod_mode=prod_mode,                # If true returns only score (without probability by models)\" + '\\n'\n",
    "        script_text += r\"                                      features=features,                  # в выходной массив добавляются занчения признаков\" + '\\n'\n",
    "        script_text += r\"                                      shap_values=shap_values)            # в выходной массив добавлюются shap значения с суффиксом _SHAP\" + '\\n'\n",
    "        script_text += r\"        return pred\" + '\\n'\n",
    "        script_text += r\"\" + '\\n'\n",
    "        script_text += r\"if __name__ == '__main__' and (sys.argv[0][-12:] != '_launcher.py'):\" + '\\n'\n",
    "        script_text += r\"\" + '\\n'\n",
    "        script_text += r\"    if (len(sys.argv) != 2) or (sys.argv[1].lower() in ['help','-help', 'h', '-h']):\" + '\\n'\n",
    "        script_text += r\"        help_msg = '\\nHELP:     To use script put as an argument path to csv data file that should be scored\\n'\" + '\\n'\n",
    "        script_text += r\"        help_msg += '\\nEXAMPLE:  python ' + sys.argv[0] + ' data_file.csv\\n'\" + '\\n'\n",
    "        script_text += r\"        help_msg += '\\nNOTICE:   Scored data file name will have suffix _scored \\n'\" + '\\n'\n",
    "        script_text += r\"        print(help_msg)\" + '\\n'\n",
    "        script_text += r\"\" + '\\n'\n",
    "        script_text += r\"    else: \" + '\\n'\n",
    "        script_text += r\"        input_file_name = sys.argv[1]\" + '\\n'\n",
    "        script_text += r\"        output_file_name = sys.argv[1][:-4] + '_scored.csv'\" + '\\n'\n",
    "        script_text += r\"\" + '\\n'\n",
    "        script_text += r\"        if os.path.exists(input_file_name):\" + '\\n'\n",
    "        script_text += r\"            with open(input_file_name, encoding='utf-8') as f:\" + '\\n'\n",
    "        script_text += r\"                h = f.readline()\" + '\\n'\n",
    "        script_text += r\"                sep = (';' if (h.count(';') > 10) else ',')\" + '\\n'\n",
    "        script_text += r\"\" + '\\n'\n",
    "        script_text += r\"            target_cols = ['FID', 'RETRO_DATE', 'ErrorCode', 'ExclusionCode', 'ID', 'PASSPORT_SER', 'PASSPORT_NUM', 'ACCT_NUM', 'Number', 'OWN_SCORE', 'DEFAULT', 'Score']\" \n",
    "        script_text += r\"\" + '\\n'\n",
    "        script_text += r\"            df = pd.read_csv(input_file_name, sep=sep, dtype={col:str for col in target_cols}, encoding='utf-8')\" + '\\n'\n",
    "        script_text += r\"\" + '\\n'\n",
    "        script_text += r\"            pred = vars()[class_name]().predict(input_type='df',\" + '\\n'        \n",
    "        script_text += r\"                                                input_data=df,\" + '\\n'        \n",
    "        script_text += r\"                                                output_type='df')\" + '\\n'\n",
    "        script_text += r\"\" + '\\n'\n",
    "        script_text += r\"            df['ErrorCode'] = None\" + '\\n'\n",
    "        script_text += r\"            df['ExclusionCode'] = None\" + '\\n'\n",
    "        script_text += r\"            df['Score'] = pred['score']\" + '\\n'\n",
    "        script_text += r\"\" + '\\n'\n",
    "        script_text += r\"            output_cols = [col for col in df.columns if col.lower() in [c.lower() for c in target_cols]]\" + '\\n'        \n",
    "        script_text += r\"            output_cols.sort(key=lambda s: [c.lower() for c in target_cols].index(s.lower()))\" + '\\n'        \n",
    "        script_text += r\"\" + '\\n'\n",
    "        script_text += r\"            df[output_cols].to_csv(output_file_name, index=False, encoding='utf-8', sep=',')\" + '\\n'\n",
    "        script_text += r\"\" + '\\n'\n",
    "        script_text += r\"            print('\\nScored data file was saved at: ' + output_file_name)\" + '\\n'\n",
    "        script_text += r\"\" + '\\n'        \n",
    "        script_text += r\"        else:\" + '\\n'        \n",
    "        script_text += r\"            print('\\nFile ' + input_file_name + ' does not exist\\n')\" + '\\n'\n",
    "        \n",
    "        prod_script_file_name = self.__class__.__name__[:-10] +'.py'\n",
    "        with open(prod_script_file_name, 'w', encoding='utf-8') as f:\n",
    "            f.write(script_text)\n",
    "        \n",
    "        print('Production script saved to file ', prod_script_file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6261960e",
   "metadata": {},
   "source": [
    "### PARAMETERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2808f748",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_DATA = '/opt/exchange/PROJECTS/BCG_GAMMA/(data)/'\n",
    "\n",
    "date_data_start = '2019-01-01 07:00:00'\n",
    "date_data_finish = '2021-03-31'\n",
    "\n",
    "date_train_start = '2019-01-01'\n",
    "date_train_finish = '2021-01-01'\n",
    "\n",
    "date_valid_start = '2019-01-01'\n",
    "date_valid_finish = '2021-04-01'\n",
    "\n",
    "df_filename = 'icl_train.csv'\n",
    "feat_filename = 'features.csv'\n",
    "\n",
    "random_state = 42\n",
    "\n",
    "max_seq_len = 90 * 24 # retrospective steps review\n",
    "\n",
    "# Seqenced data LSTM model preparation dictionary\n",
    "data_dict = {'TARGET_NAME':'qty',\n",
    "             'RNN_NUM_FEAT':['time_of_the_day', 'lune_phase', 'school_holiday', 'max_temp', 'min_temp', 'precipitation', 'days_off', 'qty'],\n",
    "             'RNN_CAT_FEAT':['weekday', 'day_month', 'week', 'month', 'hour_day'],\n",
    "             'DIR_NUM_FEAT':['time_of_the_day', 'lune_phase', 'school_holiday', 'max_temp', 'min_temp', 'precipitation', 'days_off'],\n",
    "             'DIR_CAT_FEAT':['weekday', 'day_month', 'week', 'month', 'hour_day', 'zone', 'reason'], # reason must be last feature in the list\n",
    "             'CAT':[{'NAME':'weekday',   'PROJECTION':3,  'DICT_SIZE':7},\n",
    "                    {'NAME':'day_month', 'PROJECTION':4,  'DICT_SIZE':32},\n",
    "                    {'NAME':'week',      'PROJECTION':8,  'DICT_SIZE':54},\n",
    "                    {'NAME':'month',     'PROJECTION':4,  'DICT_SIZE':13},\n",
    "                    {'NAME':'hour_day',  'PROJECTION':6,  'DICT_SIZE':24},\n",
    "                    {'NAME':'zone',      'PROJECTION':9,  'DICT_SIZE':6},\n",
    "                    {'NAME':'reason',    'PROJECTION':10, 'DICT_SIZE':10}]}\n",
    "\n",
    "reason_dict = {0: 'Перевозка плановая',\n",
    "               1: 'Перевозка экстренная',\n",
    "               2: 'без сознания',\n",
    "               3: 'боли в животе',\n",
    "               4: 'выс. давление( боли в сердце)',\n",
    "               5: 'выс. давление( голов.боль,головокруж)',\n",
    "               6: 'выс. темп.',\n",
    "               7: 'задыхается',\n",
    "               8: 'плохо',\n",
    "               9: 'плохо с сердцем'}\n",
    "\n",
    "zone_dict ={0: 'П/станция 1',\n",
    "            1: 'П/станция 2',\n",
    "            2: 'П/станция 3',\n",
    "            3: 'П/станция 6',\n",
    "            4: 'П/станция 8',\n",
    "            5: 'П/станция 9'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e0a235aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.68 s, sys: 212 ms, total: 1.9 s\n",
      "Wall time: 1.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# DATA LOADING AND PREPROCESSING\n",
    "\n",
    "df_flat=pd.concat([pd.DataFrame(data={'date':pd.date_range(start=date_data_start, end=date_data_finish, freq='H'), 'zone':zone}) for zone in range(6)], axis=0)\n",
    "\n",
    "df_feat = pd.read_csv(f'{PATH_TO_DATA}{feat_filename}', index_col=0)\n",
    "\n",
    "# Data transformation df_feat\n",
    "mapper={'Максимальная температура, С':'max_temp',\n",
    "        'Минимальная температура, С':'min_temp',\n",
    "        'Осадки, часы':'precipitation'}\n",
    "\n",
    "df_feat.rename(columns=mapper, inplace=True)\n",
    "df_feat['date'] = pd.to_datetime(df_feat['date'], format='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "# Merging with df_feat\n",
    "cols = ['weekday', 'day_month', 'week', 'month', 'hour_day', 'time_of_the_day', 'lune_phase', 'school_holiday', 'max_temp', 'min_temp', 'precipitation', 'days_off']\n",
    "int_cols = ['weekday', 'day_month', 'week', 'month', 'hour_day', 'time_of_the_day', 'school_holiday', 'max_temp', 'min_temp', 'precipitation', 'days_off']\n",
    "\n",
    "for col in cols:\n",
    "    df_flat[col] = df_flat['date'].map(df_feat.set_index('date')[col])\n",
    "for col in int_cols:\n",
    "    df_flat[col] = df_flat[col].fillna(0).astype('int8')\n",
    "\n",
    "# DF_start processing\n",
    "df_start=pd.read_csv(f'{PATH_TO_DATA}{df_filename}')\n",
    "df_start = df_start[['date', 'zone'] + list([reason_dict[key] for key in reason_dict.keys()])]\n",
    "df_start = df_start[df_start['zone'].isin([zone_dict[key] for key in zone_dict.keys()])]\n",
    "df_start['zone'] = df_start['zone'].map({zone_dict[key]:key for key in zone_dict.keys()})\n",
    "df_start.rename(columns={reason_dict[key]:key for key in reason_dict.keys()}, inplace=True)\n",
    "df_start['date'] = pd.to_datetime(df_start['date'], format='%Y-%m-%d %H:%M:%S')\n",
    "    \n",
    "# Merging with df_start\n",
    "df_flat = pd.merge(left=df_flat,\n",
    "                   right=df_start,\n",
    "                   how='left',\n",
    "                   on=['date', 'zone'])\n",
    "df_flat.loc[:,'zone':] = df_flat.loc[:,'zone':].fillna(-1).astype('int8')\n",
    "\n",
    "del(df_feat, df_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7204d8f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.43 s, sys: 662 ms, total: 7.09 s\n",
      "Wall time: 7.09 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# df and df_ preparartion\n",
    "feat_lst = data_dict['DIR_NUM_FEAT'] + data_dict['DIR_CAT_FEAT']\n",
    "df_lst = []\n",
    "\n",
    "for col in reason_dict.keys():\n",
    "    df_lst.append(df_flat[['date'] + feat_lst[:-1]])\n",
    "    df_lst[-1][data_dict['TARGET_NAME']] = df_flat[col]\n",
    "    df_lst[-1]['reason'] = col\n",
    "    df_lst[-1]['reason'] = df_lst[-1]['reason'].astype('int16')\n",
    "df = pd.concat(df_lst, axis=0)[['date'] + feat_lst + [data_dict['TARGET_NAME']]].reset_index()\n",
    "df.rename(columns={'index':'index_flat'}, inplace=True)\n",
    "\n",
    "del(df_lst)\n",
    "\n",
    "# forming df for training\n",
    "\n",
    "df_ = df[df['date'] > df_flat['date'].drop_duplicates().sort_values().iloc[max_seq_len]]\n",
    "df_ = df_.reset_index()\n",
    "\n",
    "# Converting numeric features to lists\n",
    "for col in data_dict['RNN_NUM_FEAT']:\n",
    "    df[col] = df[col].map(lambda x:[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0d703a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_['qty'] = (df_['qty'] > 0).astype('int8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cb7fa796",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>index_flat</th>\n",
       "      <th>date</th>\n",
       "      <th>time_of_the_day</th>\n",
       "      <th>lune_phase</th>\n",
       "      <th>school_holiday</th>\n",
       "      <th>max_temp</th>\n",
       "      <th>min_temp</th>\n",
       "      <th>precipitation</th>\n",
       "      <th>days_off</th>\n",
       "      <th>weekday</th>\n",
       "      <th>day_month</th>\n",
       "      <th>week</th>\n",
       "      <th>month</th>\n",
       "      <th>hour_day</th>\n",
       "      <th>zone</th>\n",
       "      <th>reason</th>\n",
       "      <th>qty</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2161</td>\n",
       "      <td>2161</td>\n",
       "      <td>2019-04-01 08:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2162</td>\n",
       "      <td>2162</td>\n",
       "      <td>2019-04-01 09:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2163</td>\n",
       "      <td>2163</td>\n",
       "      <td>2019-04-01 10:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2164</td>\n",
       "      <td>2164</td>\n",
       "      <td>2019-04-01 11:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2165</td>\n",
       "      <td>2165</td>\n",
       "      <td>2019-04-01 12:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>4</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1050775</th>\n",
       "      <td>1180435</td>\n",
       "      <td>118039</td>\n",
       "      <td>2021-03-30 20:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>-4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>30</td>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "      <td>20</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1050776</th>\n",
       "      <td>1180436</td>\n",
       "      <td>118040</td>\n",
       "      <td>2021-03-30 21:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>-4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>30</td>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "      <td>21</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1050777</th>\n",
       "      <td>1180437</td>\n",
       "      <td>118041</td>\n",
       "      <td>2021-03-30 22:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>-4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>30</td>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "      <td>22</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1050778</th>\n",
       "      <td>1180438</td>\n",
       "      <td>118042</td>\n",
       "      <td>2021-03-30 23:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>-4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>30</td>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "      <td>23</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1050779</th>\n",
       "      <td>1180439</td>\n",
       "      <td>118043</td>\n",
       "      <td>2021-03-31 00:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>31</td>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1050780 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           index  index_flat                date  time_of_the_day  lune_phase  \\\n",
       "0           2161        2161 2019-04-01 08:00:00                1           0   \n",
       "1           2162        2162 2019-04-01 09:00:00                1           0   \n",
       "2           2163        2163 2019-04-01 10:00:00                1           0   \n",
       "3           2164        2164 2019-04-01 11:00:00                1           0   \n",
       "4           2165        2165 2019-04-01 12:00:00                1           0   \n",
       "...          ...         ...                 ...              ...         ...   \n",
       "1050775  1180435      118039 2021-03-30 20:00:00                0           0   \n",
       "1050776  1180436      118040 2021-03-30 21:00:00                0           0   \n",
       "1050777  1180437      118041 2021-03-30 22:00:00                0           0   \n",
       "1050778  1180438      118042 2021-03-30 23:00:00                0           0   \n",
       "1050779  1180439      118043 2021-03-31 00:00:00                0           0   \n",
       "\n",
       "         school_holiday  max_temp  min_temp  precipitation  days_off  weekday  \\\n",
       "0                     0         5        -1              0         0        0   \n",
       "1                     0         5        -1              0         0        0   \n",
       "2                     0         5        -1              0         0        0   \n",
       "3                     0         5        -1              0         0        0   \n",
       "4                     0         5        -1              0         0        0   \n",
       "...                 ...       ...       ...            ...       ...      ...   \n",
       "1050775               0         6        -4              0         0        1   \n",
       "1050776               0         6        -4              0         0        1   \n",
       "1050777               0         6        -4              0         0        1   \n",
       "1050778               0         6        -4              0         0        1   \n",
       "1050779               0         4         0             12         0        2   \n",
       "\n",
       "         day_month  week  month  hour_day  zone  reason  qty  \n",
       "0                1    14      4         8     0       0    1  \n",
       "1                1    14      4         9     0       0    1  \n",
       "2                1    14      4        10     0       0    1  \n",
       "3                1    14      4        11     0       0    1  \n",
       "4                1    14      4        12     0       0    0  \n",
       "...            ...   ...    ...       ...   ...     ...  ...  \n",
       "1050775         30    13      3        20     5       9    0  \n",
       "1050776         30    13      3        21     5       9    0  \n",
       "1050777         30    13      3        22     5       9    0  \n",
       "1050778         30    13      3        23     5       9    0  \n",
       "1050779         31    13      3         0     5       9    0  \n",
       "\n",
       "[1050780 rows x 18 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bd7ec009",
   "metadata": {},
   "outputs": [],
   "source": [
    "# General\n",
    "TARGET_NAME = \"qty\"\n",
    "cat_columns = ['weekday', 'zone', 'reason']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "99979de7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 16.1 ms, sys: 13 µs, total: 16.1 ms\n",
      "Wall time: 16 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# подготовка моделей\n",
    "catbm = catb.CatBoostClassifier(eval_metric='AUC',\n",
    "                                silent=True,\n",
    "                                iterations=1000,\n",
    "                                random_state=21)\n",
    "\n",
    "models = {'catbm':catbm}\n",
    "\n",
    "model = model_dev(df=df_.drop(columns=['index', 'index_flat', 'date']),                                # датасет\n",
    "                   TARGET_NAME=TARGET_NAME,                    # Имя столбца с целевой переменной\n",
    "                   models=models,                              # словарь моделей (обязательно  должны быть методы fit, predict)   \n",
    "                   cat_columns=cat_columns,                    # Названия столбцов с категориальными признакми\n",
    "                   scaler_columns=[],              # Названия столбцов по которым необходимо проводить масштабирование\n",
    "                   scaler_type='std',                          # Тип масштабирования признаков ('none', 'std', 'norm')\n",
    "                   target_class_ratio=-1,                     # коэффициент для балансровки соотношения классов целевой переменной\n",
    "                   downsample=False,                            # при щначении True балансирвка осуществляется через downsampling \n",
    "                   SKF_splits=5,                               # кол-во фолдов для валидации\n",
    "                   sample_weight=True,                         # указание веса каждого наблюдения для модели CatboostClassifier \n",
    "                   random_state=42)                            # Random_State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "53feb468",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "FOLD 1 REPORT\n",
      "\u001b[4mModel                                        Qty     1 %  f1 score    Recall  Precission   ROC AUC      Gini\u001b[0m\n",
      "catbm_1_valid                             210155  19.09%    0.4634    0.7283      0.3398    0.7758    0.5516\n",
      "        train                             840625  19.09%    0.4682    0.7359      0.3433    0.7815     0.563\n",
      "\n",
      "FOLD 2 REPORT\n",
      "\u001b[4mModel                                        Qty     1 %  f1 score    Recall  Precission   ROC AUC      Gini\u001b[0m\n",
      "catbm_2_valid                             210155  19.09%    0.4615    0.7278      0.3379    0.7734    0.5468\n",
      "        train                             840625  19.09%    0.4678     0.738      0.3425    0.7823    0.5646\n",
      "\n",
      "FOLD 3 REPORT\n",
      "\u001b[4mModel                                        Qty     1 %  f1 score    Recall  Precission   ROC AUC      Gini\u001b[0m\n",
      "catbm_3_valid                             210155  19.09%    0.4613    0.7247      0.3384    0.7739    0.5478\n",
      "        train                             840625  19.09%    0.4685    0.7365      0.3435    0.7819    0.5638\n",
      "\n",
      "FOLD 4 REPORT\n",
      "\u001b[4mModel                                        Qty     1 %  f1 score    Recall  Precission   ROC AUC      Gini\u001b[0m\n",
      "catbm_4_valid                             210155  19.09%    0.4617    0.7242      0.3388    0.7727    0.5454\n",
      "        train                             840625  19.09%    0.4687    0.7353       0.344    0.7822    0.5644\n",
      "\n",
      "FOLD 5 REPORT\n",
      "\u001b[4mModel                                        Qty     1 %  f1 score    Recall  Precission   ROC AUC      Gini\u001b[0m\n",
      "catbm_5_valid                             210160  19.09%    0.4599    0.7219      0.3374    0.7729    0.5458\n",
      "        train                             840620  19.09%    0.4692    0.7349      0.3446    0.7822    0.5644\n",
      "\n",
      "FINAL REPORT (5 folds AVG)\n",
      "\u001b[4mModel                                        Qty     1 %  f1 score    Recall  Precission   ROC AUC      Gini\u001b[0m\n",
      "catbm                                    1050780  19.09%    0.4615    0.7254      0.3384    0.7737    0.5474\n",
      "MIX RESULT                               1050780  19.09%    0.4615    0.7254      0.3384    0.7737    0.5474\n"
     ]
    }
   ],
   "source": [
    "model.fit(train_report=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e7b81699",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to file: prob_model_boosting\n"
     ]
    }
   ],
   "source": [
    "model.save(file_name='prob_model_boosting', prod_mode=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e724766",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "926c6bf4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07f2ffc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c41c4ed1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bec4b55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b18e250d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9873cc62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65baff35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f049a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b36549c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182a57ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9229150e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
