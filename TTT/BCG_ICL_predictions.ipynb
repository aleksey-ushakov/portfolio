{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a2adba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.python.keras.layers import deserialize, serialize\n",
    "from tensorflow.keras.layers import Input, Embedding, LayerNormalization, concatenate, LSTM, BatchNormalization, Dense, Reshape\n",
    "from tensorflow.python.keras.saving import saving_utils\n",
    "from tensorflow.random import set_seed\n",
    "from tensorflow.keras.optimizers import Nadam\n",
    "from sklearn.utils import shuffle\n",
    "from itertools import product\n",
    "\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import os, sys\n",
    "import datetime\n",
    "import json\n",
    "import dill as pickle #более мощная библиотека позволяющая сохранять функции\n",
    "import random\n",
    "import copy\n",
    "\n",
    "from IPython.display import display\n",
    "\n",
    "from scipy.stats import kstest, anderson #lilliefors\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split, ShuffleSplit, cross_val_score, learning_curve\n",
    "from sklearn.model_selection import KFold, GridSearchCV, RandomizedSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import classification_report, f1_score, precision_score, recall_score, r2_score, roc_auc_score\n",
    "from sklearn.utils import resample\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "\n",
    "import catboost as catb\n",
    "\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "%matplotlib inline\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "now_str = lambda : datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e632dc07",
   "metadata": {},
   "source": [
    "### PARAMETERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c2d0ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_DATA = '/opt/exchange/PROJECTS/BCG_GAMMA/(data)/'\n",
    "\n",
    "date_data_start = '2019-01-01 07:00:00'\n",
    "date_data_finish = '2021-03-31'\n",
    "\n",
    "date_train_start = '2019-01-01'\n",
    "date_train_finish = '2021-01-01'\n",
    "\n",
    "date_valid_start = '2020-12-01'\n",
    "date_valid_finish = '2020-12-31'\n",
    "\n",
    "date_test_start = '2021-01-01'\n",
    "date_test_finish = '2021-03-31'\n",
    "\n",
    "df_filename = 'icl_train.csv'\n",
    "feat_filename = 'features.csv'\n",
    "\n",
    "random_state = 42\n",
    "\n",
    "max_seq_len = 90 * 24 # retrospective steps review\n",
    "\n",
    "# Seqenced data LSTM model preparation dictionary\n",
    "data_dict = {'TARGET_NAME':'qty',\n",
    "             'RNN_NUM_FEAT':['time_of_the_day', 'lune_phase', 'school_holiday', 'max_temp', 'min_temp', 'precipitation', 'days_off', 'qty'],\n",
    "             'RNN_CAT_FEAT':['weekday', 'day_month', 'week', 'month', 'hour_day'],\n",
    "             'DIR_NUM_FEAT':['time_of_the_day', 'lune_phase', 'school_holiday', 'max_temp', 'min_temp', 'precipitation', 'days_off'],\n",
    "             'DIR_CAT_FEAT':['weekday', 'day_month', 'week', 'month', 'hour_day', 'zone', 'reason'], # reason must be last feature in the list\n",
    "             'CAT':[{'NAME':'weekday',   'PROJECTION':3,  'DICT_SIZE':7},\n",
    "                    {'NAME':'day_month', 'PROJECTION':4,  'DICT_SIZE':32},\n",
    "                    {'NAME':'week',      'PROJECTION':8,  'DICT_SIZE':54},\n",
    "                    {'NAME':'month',     'PROJECTION':4,  'DICT_SIZE':13},\n",
    "                    {'NAME':'hour_day',  'PROJECTION':6,  'DICT_SIZE':24},\n",
    "                    {'NAME':'zone',      'PROJECTION':9,  'DICT_SIZE':6},\n",
    "                    {'NAME':'reason',    'PROJECTION':10, 'DICT_SIZE':10}]}\n",
    "\n",
    "reason_dict = {0: 'Перевозка плановая',\n",
    "               1: 'Перевозка экстренная',\n",
    "               2: 'без сознания',\n",
    "               3: 'боли в животе',\n",
    "               4: 'выс. давление( боли в сердце)',\n",
    "               5: 'выс. давление( голов.боль,головокруж)',\n",
    "               6: 'выс. темп.',\n",
    "               7: 'задыхается',\n",
    "               8: 'плохо',\n",
    "               9: 'плохо с сердцем'}\n",
    "\n",
    "zone_dict ={0: 'П/станция 1',\n",
    "            1: 'П/станция 2',\n",
    "            2: 'П/станция 3',\n",
    "            3: 'П/станция 6',\n",
    "            4: 'П/станция 8',\n",
    "            5: 'П/станция 9'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e22e374",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.82 s, sys: 199 ms, total: 2.02 s\n",
      "Wall time: 2.02 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# DATA LOADING AND PREPROCESSING\n",
    "\n",
    "df_flat=pd.concat([pd.DataFrame(data={'date':pd.date_range(start=date_data_start, end=date_data_finish, freq='H'), 'zone':zone}) for zone in range(6)], axis=0)\n",
    "\n",
    "df_feat = pd.read_csv(f'{PATH_TO_DATA}{feat_filename}', index_col=0)\n",
    "\n",
    "# Data transformation df_feat\n",
    "mapper={'Максимальная температура, С':'max_temp',\n",
    "        'Минимальная температура, С':'min_temp',\n",
    "        'Осадки, часы':'precipitation'}\n",
    "\n",
    "df_feat.rename(columns=mapper, inplace=True)\n",
    "df_feat['date'] = pd.to_datetime(df_feat['date'], format='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "# Merging with df_feat\n",
    "cols = ['weekday', 'day_month', 'week', 'month', 'hour_day', 'time_of_the_day', 'lune_phase', 'school_holiday', 'max_temp', 'min_temp', 'precipitation', 'days_off']\n",
    "int_cols = ['weekday', 'day_month', 'week', 'month', 'hour_day', 'time_of_the_day', 'school_holiday', 'max_temp', 'min_temp', 'precipitation', 'days_off']\n",
    "\n",
    "for col in cols:\n",
    "    df_flat[col] = df_flat['date'].map(df_feat.set_index('date')[col])\n",
    "for col in int_cols:\n",
    "    df_flat[col] = df_flat[col].fillna(0).astype('int8')\n",
    "\n",
    "# DF_start processing\n",
    "df_start=pd.read_csv(f'{PATH_TO_DATA}{df_filename}')\n",
    "df_start = df_start[['date', 'zone'] + list([reason_dict[key] for key in reason_dict.keys()])]\n",
    "df_start = df_start[df_start['zone'].isin([zone_dict[key] for key in zone_dict.keys()])]\n",
    "df_start['zone'] = df_start['zone'].map({zone_dict[key]:key for key in zone_dict.keys()})\n",
    "df_start.rename(columns={reason_dict[key]:key for key in reason_dict.keys()}, inplace=True)\n",
    "df_start['date'] = pd.to_datetime(df_start['date'], format='%Y-%m-%d %H:%M:%S')\n",
    "    \n",
    "# Merging with df_start\n",
    "df_flat = pd.merge(left=df_flat,\n",
    "                   right=df_start,\n",
    "                   how='left',\n",
    "                   on=['date', 'zone'])\n",
    "df_flat.loc[:,'zone':] = df_flat.loc[:,'zone':].fillna(-1).astype('int8')\n",
    "\n",
    "del(df_feat, df_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b0643db9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.8 s, sys: 395 ms, total: 7.2 s\n",
      "Wall time: 7.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# df and df_ preparartion\n",
    "feat_lst = data_dict['DIR_NUM_FEAT'] + data_dict['DIR_CAT_FEAT']\n",
    "df_lst = []\n",
    "\n",
    "for col in reason_dict.keys():\n",
    "    df_lst.append(df_flat[['date'] + feat_lst[:-1]])\n",
    "    df_lst[-1][data_dict['TARGET_NAME']] = df_flat[col]\n",
    "    df_lst[-1]['reason'] = col\n",
    "    df_lst[-1]['reason'] = df_lst[-1]['reason'].astype('int16')\n",
    "df = pd.concat(df_lst, axis=0)[['date'] + feat_lst + [data_dict['TARGET_NAME']]].reset_index()\n",
    "df.rename(columns={'index':'index_flat'}, inplace=True)\n",
    "\n",
    "del(df_lst)\n",
    "\n",
    "# forming df for training\n",
    "\n",
    "df_ = df[df['date'] > df_flat['date'].drop_duplicates().sort_values().iloc[max_seq_len]]\n",
    "df_ = df_.reset_index()\n",
    "\n",
    "df_test = df[df['date'] >= pd.to_datetime(date_test_start, format='%Y-%m-%d')]\n",
    "df_test['zone_name'] = df_test['zone'].map(zone_dict)\n",
    "df_test['reason_name'] = df_test['reason'].map(reason_dict)\n",
    "\n",
    "df_valid = df[(df['date'] >= pd.to_datetime(date_valid_start, format='%Y-%m-%d')) & (df['date'] <= pd.to_datetime(date_valid_finish, format='%Y-%m-%d'))]\n",
    "df_valid['zone_name'] = df_valid['zone'].map(zone_dict)\n",
    "df_valid['reason_name'] = df_valid['reason'].map(reason_dict)\n",
    "\n",
    "# Converting numeric features to lists\n",
    "for col in data_dict['RNN_NUM_FEAT']:\n",
    "    df[col] = df[col].map(lambda x:[x])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d35d4d",
   "metadata": {},
   "source": [
    "### MODEL BUILDING & GENERATOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2767544a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# batch generation\n",
    "class BatchGenerator:\n",
    "    \"\"\"\n",
    "    Docstring\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 df=None, # Historical data\n",
    "                 df_=None, # data to for training or prediction\n",
    "                 batch_size=32,\n",
    "                 seq_len = None,\n",
    "                 order_col='date',\n",
    "                 data_dict=None,\n",
    "                 random_state=42):\n",
    "        \"\"\"\n",
    "        Docstring\n",
    "        \"\"\"\n",
    "        self.df = df\n",
    "        self.df_ = df_\n",
    "        self.batch_size = batch_size\n",
    "        self.seq_len = seq_len\n",
    "        self.random_state = random_state\n",
    "        self.data_dict = data_dict\n",
    "        self.steps = (df_.shape[0] // batch_size) if ((df_.shape[0] % batch_size) == 0) else ((df_.shape[0] // batch_size) + 1)\n",
    "\n",
    "    def gen(self, infinit_mode=False, train_mode=False):\n",
    "\n",
    "        rnn_feat = data_dict['RNN_NUM_FEAT'] + data_dict['RNN_CAT_FEAT']\n",
    "        dir_feat = data_dict['DIR_NUM_FEAT'] + data_dict['DIR_CAT_FEAT']\n",
    "        \n",
    "        # main cycle\n",
    "        while True:\n",
    "            np.random.seed(self.random_state)\n",
    "            \n",
    "            if train_mode:\n",
    "                df_ = self.df_.sample(n=self.df_.shape[0], random_state=self.random_state)\n",
    "\n",
    "            #df_['batch_num'] = (np.ones((batch_size, df_flat.shape[0] // 32 +1)) * np.arange(df_flat.shape[0] // 32 +1)).astype(int).T.flatten()[:df_flat.shape[0]]\n",
    "\n",
    "            # Batch generation\n",
    "            for i in range(self.steps):\n",
    "                batch_data = []\n",
    "                for j in range(self.batch_size):\n",
    "                    ind = self.df_.iloc[i * self.batch_size + j, 0]\n",
    "                    reason = self.df_.iloc[i * self.batch_size + j]['reason']\n",
    "                    batch_data.append([self.df.loc[ind - self.seq_len: ind - 1, ['zone'] + [col]].groupby(by='zone').agg(list)[col].values.tolist()[0] for col in rnn_feat])\n",
    "                x = [np.array([row[n] for row in batch_data]) for n in range(len(rnn_feat))] + [self.df_.iloc[i * self.batch_size: (i+1) * self.batch_size][col].values for col in dir_feat]\n",
    "\n",
    "                if train_mode:\n",
    "                    y = self.df_.iloc[i * self.batch_size: (i + 1) * self.batch_size][self.data_dict['TARGET_NAME']].values\n",
    "                    #sample_weight = self.df_.iloc[i * batch_size: (i+1) * batch_size]['sample_weight'].values\n",
    "                    yield x, y #, sample_weight\n",
    "                else:\n",
    "                    yield x\n",
    "            \n",
    "            self.random_state += 1\n",
    "\n",
    "            if not infinit_mode:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "09378bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_model = pickle.load(open('prob_model', 'rb'))\n",
    "regr_model = pickle.load(open('regr_model', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d96a6aa4",
   "metadata": {},
   "source": [
    "### VALIDATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af2560a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_valid['prob'] = None\n",
    "df_valid['regr'] = None\n",
    "df_valid['pred'] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6944bee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_valid.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0919dff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "print(f'{now_str()} start_time')\n",
    "test_iter_qty = df_valid['date'].drop_duplicates().shape[0]\n",
    "for i, test_date in enumerate(df_valid['date'].drop_duplicates().values):\n",
    "    print('\\r'* 100 + f'{now_str()} {i} / {test_iter_qty}', end='')\n",
    "    df_valid_date = df_valid.loc[df_valid['date']==test_date]\n",
    "    \n",
    "    test_gen = BatchGenerator(df=df, # Historical data\n",
    "                              df_=df_valid_date, # data to for training or prediction\n",
    "                              batch_size=60,\n",
    "                              seq_len=24*28,\n",
    "                              order_col='date',\n",
    "                              data_dict=data_dict).gen(infinit_mode=False, train_mode=False)\n",
    "    pred_proba = prob_model.predict(test_gen)\n",
    "    \n",
    "    test_gen = BatchGenerator(df=df, # Historical data\n",
    "                          df_=df_valid_date, # data to for training or prediction\n",
    "                          batch_size=60,\n",
    "                          seq_len=24*28,\n",
    "                          order_col='date',\n",
    "                          data_dict=data_dict).gen(infinit_mode=False, train_mode=False)\n",
    "    pred_regr = regr_model.predict(test_gen)\n",
    "    \n",
    "    df_valid.loc[df_valid_date.index, 'prob'] = pred_proba\n",
    "    df_valid.loc[df_valid_date.index, 'regr'] = pred_regr\n",
    "    df_valid.loc[df_valid_date.index, 'pred'] = (df_valid.loc[df_valid_date.index, 'prob'] > 0.4) * df_valid.loc[df_valid_date.index, 'regr']  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97049c88",
   "metadata": {},
   "source": [
    "### PREDICTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "58339284",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['prob'] = None\n",
    "df_test['regr'] = None\n",
    "df_test['pred'] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "06f31d5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index_flat</th>\n",
       "      <th>date</th>\n",
       "      <th>time_of_the_day</th>\n",
       "      <th>lune_phase</th>\n",
       "      <th>school_holiday</th>\n",
       "      <th>max_temp</th>\n",
       "      <th>min_temp</th>\n",
       "      <th>precipitation</th>\n",
       "      <th>days_off</th>\n",
       "      <th>weekday</th>\n",
       "      <th>day_month</th>\n",
       "      <th>week</th>\n",
       "      <th>month</th>\n",
       "      <th>hour_day</th>\n",
       "      <th>zone</th>\n",
       "      <th>reason</th>\n",
       "      <th>qty</th>\n",
       "      <th>zone_name</th>\n",
       "      <th>reason_name</th>\n",
       "      <th>prob</th>\n",
       "      <th>regr</th>\n",
       "      <th>pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17537</th>\n",
       "      <td>17537</td>\n",
       "      <td>2021-01-01 00:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-7</td>\n",
       "      <td>-11</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>53</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>П/станция 1</td>\n",
       "      <td>Перевозка плановая</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17538</th>\n",
       "      <td>17538</td>\n",
       "      <td>2021-01-01 01:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-7</td>\n",
       "      <td>-11</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>53</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>П/станция 1</td>\n",
       "      <td>Перевозка плановая</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17539</th>\n",
       "      <td>17539</td>\n",
       "      <td>2021-01-01 02:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-7</td>\n",
       "      <td>-11</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>53</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>П/станция 1</td>\n",
       "      <td>Перевозка плановая</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17540</th>\n",
       "      <td>17540</td>\n",
       "      <td>2021-01-01 03:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-7</td>\n",
       "      <td>-11</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>53</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>П/станция 1</td>\n",
       "      <td>Перевозка плановая</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17541</th>\n",
       "      <td>17541</td>\n",
       "      <td>2021-01-01 04:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-7</td>\n",
       "      <td>-11</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>53</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>П/станция 1</td>\n",
       "      <td>Перевозка плановая</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1180435</th>\n",
       "      <td>118039</td>\n",
       "      <td>2021-03-30 20:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>-4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>30</td>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "      <td>20</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>-1</td>\n",
       "      <td>П/станция 9</td>\n",
       "      <td>плохо с сердцем</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1180436</th>\n",
       "      <td>118040</td>\n",
       "      <td>2021-03-30 21:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>-4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>30</td>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "      <td>21</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>-1</td>\n",
       "      <td>П/станция 9</td>\n",
       "      <td>плохо с сердцем</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1180437</th>\n",
       "      <td>118041</td>\n",
       "      <td>2021-03-30 22:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>-4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>30</td>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "      <td>22</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>-1</td>\n",
       "      <td>П/станция 9</td>\n",
       "      <td>плохо с сердцем</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1180438</th>\n",
       "      <td>118042</td>\n",
       "      <td>2021-03-30 23:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>-4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>30</td>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "      <td>23</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>-1</td>\n",
       "      <td>П/станция 9</td>\n",
       "      <td>плохо с сердцем</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1180439</th>\n",
       "      <td>118043</td>\n",
       "      <td>2021-03-31 00:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>31</td>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>-1</td>\n",
       "      <td>П/станция 9</td>\n",
       "      <td>плохо с сердцем</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>128220 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         index_flat                date  time_of_the_day  lune_phase  \\\n",
       "17537         17537 2021-01-01 00:00:00                0           0   \n",
       "17538         17538 2021-01-01 01:00:00                0           0   \n",
       "17539         17539 2021-01-01 02:00:00                0           0   \n",
       "17540         17540 2021-01-01 03:00:00                0           0   \n",
       "17541         17541 2021-01-01 04:00:00                0           0   \n",
       "...             ...                 ...              ...         ...   \n",
       "1180435      118039 2021-03-30 20:00:00                0           0   \n",
       "1180436      118040 2021-03-30 21:00:00                0           0   \n",
       "1180437      118041 2021-03-30 22:00:00                0           0   \n",
       "1180438      118042 2021-03-30 23:00:00                0           0   \n",
       "1180439      118043 2021-03-31 00:00:00                0           0   \n",
       "\n",
       "         school_holiday  max_temp  min_temp  precipitation  days_off  weekday  \\\n",
       "17537                 1        -7       -11              0         1        4   \n",
       "17538                 1        -7       -11              0         1        4   \n",
       "17539                 1        -7       -11              0         1        4   \n",
       "17540                 1        -7       -11              0         1        4   \n",
       "17541                 1        -7       -11              0         1        4   \n",
       "...                 ...       ...       ...            ...       ...      ...   \n",
       "1180435               0         6        -4              0         0        1   \n",
       "1180436               0         6        -4              0         0        1   \n",
       "1180437               0         6        -4              0         0        1   \n",
       "1180438               0         6        -4              0         0        1   \n",
       "1180439               0         4         0             12         0        2   \n",
       "\n",
       "         day_month  week  month  hour_day  zone  reason  qty    zone_name  \\\n",
       "17537            1    53      1         0     0       0   -1  П/станция 1   \n",
       "17538            1    53      1         1     0       0   -1  П/станция 1   \n",
       "17539            1    53      1         2     0       0   -1  П/станция 1   \n",
       "17540            1    53      1         3     0       0   -1  П/станция 1   \n",
       "17541            1    53      1         4     0       0   -1  П/станция 1   \n",
       "...            ...   ...    ...       ...   ...     ...  ...          ...   \n",
       "1180435         30    13      3        20     5       9   -1  П/станция 9   \n",
       "1180436         30    13      3        21     5       9   -1  П/станция 9   \n",
       "1180437         30    13      3        22     5       9   -1  П/станция 9   \n",
       "1180438         30    13      3        23     5       9   -1  П/станция 9   \n",
       "1180439         31    13      3         0     5       9   -1  П/станция 9   \n",
       "\n",
       "                reason_name  prob  regr  pred  \n",
       "17537    Перевозка плановая  None  None  None  \n",
       "17538    Перевозка плановая  None  None  None  \n",
       "17539    Перевозка плановая  None  None  None  \n",
       "17540    Перевозка плановая  None  None  None  \n",
       "17541    Перевозка плановая  None  None  None  \n",
       "...                     ...   ...   ...   ...  \n",
       "1180435     плохо с сердцем  None  None  None  \n",
       "1180436     плохо с сердцем  None  None  None  \n",
       "1180437     плохо с сердцем  None  None  None  \n",
       "1180438     плохо с сердцем  None  None  None  \n",
       "1180439     плохо с сердцем  None  None  None  \n",
       "\n",
       "[128220 rows x 22 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f55037f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-06-27 15:27:21 start_time\n",
      "2021-06-27 18:18:24 845 / 2137"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(f'{now_str()} start_time')\n",
    "test_iter_qty = df_test['date'].drop_duplicates().shape[0]\n",
    "for i, test_date in enumerate(df_test['date'].drop_duplicates().values):\n",
    "    print('\\r'* 100 + f'{now_str()} {i} / {test_iter_qty}', end='')\n",
    "    df_test_date = df_test.loc[df_test['date']==test_date]\n",
    "    \n",
    "    test_gen = BatchGenerator(df=df, # Historical data\n",
    "                              df_=df_test_date, # data to for training or prediction\n",
    "                              batch_size=60,\n",
    "                              seq_len=24*28,\n",
    "                              order_col='date',\n",
    "                              data_dict=data_dict).gen(infinit_mode=False, train_mode=False)\n",
    "    pred_proba = prob_model.predict(test_gen)\n",
    "    \n",
    "    test_gen = BatchGenerator(df=df, # Historical data\n",
    "                          df_=df_test_date, # data to for training or prediction\n",
    "                          batch_size=60,\n",
    "                          seq_len=24*28,\n",
    "                          order_col='date',\n",
    "                          data_dict=data_dict).gen(infinit_mode=False, train_mode=False)\n",
    "    pred_regr = regr_model.predict(test_gen)\n",
    "    \n",
    "    df_test.loc[df_test_date.index, 'prob'] = pred_proba\n",
    "    df_test.loc[df_test_date.index, 'regr'] = pred_regr\n",
    "    df_test.loc[df_test_date.index, 'pred'] = (df_test.loc[df_test_date.index, 'prob'] > 0.4) * df_test.loc[df_test_date.index, 'regr']  \n",
    "    \n",
    "    df.loc[df_test_date.index, 'qty'] = df_test.loc[df_test_date.index, 'pred'].map(lambda x: [round(x)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b3fec8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609dbb82",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b5943be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cceac92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "415de0e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a11ec362",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67179743",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc41fa3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e739b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc2854b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e23c2d9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9242c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## MultiTarget Two Stage Model\n",
    "\n",
    "class MTModelDev:\n",
    "    \n",
    "    '''\n",
    "    Docstring\n",
    "    '''\n",
    "    \n",
    "    def __init__(self,\n",
    "                 train_file_list=[],\n",
    "                 valid_file_list=[],\n",
    "                 data_dict=None,\n",
    "                 random_state=42,\n",
    "                 separate_training_data=False,\n",
    "                 stage_1_model_optimizer=None,\n",
    "                 stage_1_model_lstm_units=32,\n",
    "                 stage_1_model_final_dense_units=128,\n",
    "                 stage_1_model_random_state=33,\n",
    "                 stage_2_model_eval_metric='AUC',\n",
    "                 stage_2_model_iterations=1000,\n",
    "                 stage_2_model_SKF_splits=5,\n",
    "                 stage_2_model_class_weight=None,\n",
    "                 stage_2_model_random_state=21):\n",
    "        \n",
    "        '''\n",
    "        Docstring\n",
    "        '''\n",
    "        self.data_dict = data_dict\n",
    "        self.TARGET_NAME = data_dict['TARGET_NAME']\n",
    "        \n",
    "        self.train_file_list = train_file_list\n",
    "        self.valid_file_list = valid_file_list\n",
    "        self.random_state = random_state\n",
    "        self.separate_training_data = separate_training_data\n",
    "        \n",
    "        self.stage_1_model_optimizer = stage_1_model_optimizer\n",
    "        self.stage_1_model_lstm_units = stage_1_model_lstm_units\n",
    "        self.stage_1_model_final_dense_units = stage_1_model_final_dense_units\n",
    "        self.stage_1_model_random_state=stage_1_model_random_state\n",
    "        self.stage_2_model_eval_metric=stage_2_model_eval_metric\n",
    "        self.stage_2_model_iterations=stage_2_model_iterations\n",
    "        self.stage_2_model_SKF_splits=stage_2_model_SKF_splits\n",
    "        self.stage_2_model_class_weight=stage_2_model_class_weight        \n",
    "        self.stage_2_model_random_state=stage_2_model_random_state\n",
    "        \n",
    "        # Class/Sample weight parameters calculation\n",
    "        df_weight = pd.concat([pd.read_parquet(file_name, columns=[TARGET_NAME, 'TARGET']) for file_name in train_file_list], axis=0).reset_index(drop=True)\n",
    "        self.stage_1_model_class_weight = (0.5 / df_weight[self.TARGET_NAME].astype(int).value_counts(normalize=True)).to_dict()\n",
    "        \n",
    "        self.target_weight = 1 / df_weight['TARGET'].value_counts(normalize=True) / df_weight['TARGET'].nunique()\n",
    "        self.true_class_weight = 1 / df_weight.groupby('TARGET')[TARGET_NAME].mean() - 1\n",
    "        \n",
    "        if type(self.stage_2_model_class_weight)==type(None):\n",
    "            self.stage_2_model_class_weight = self.stage_1_model_class_weight\n",
    "        del(df_weight)\n",
    "        \n",
    "        self.stage_1_full_model = None\n",
    "        self.stage_1_model = None\n",
    "        self.stage_2_model = None\n",
    "        \n",
    "        # patch for keras.model to make it picklable\n",
    "        def unpack(model, training_config, weights):\n",
    "            restored_model = deserialize(model)\n",
    "            if training_config is not None:\n",
    "                restored_model.compile(\n",
    "                    **saving_utils.compile_args_from_training_config(\n",
    "                        training_config\n",
    "                    )\n",
    "                )\n",
    "            restored_model.set_weights(weights)\n",
    "            return restored_model\n",
    "\n",
    "        # Hotfix function\n",
    "        def make_keras_picklable():\n",
    "            def __reduce__(self):\n",
    "                model_metadata = saving_utils.model_metadata(self)\n",
    "                training_config = model_metadata.get(\"training_config\", None)\n",
    "                model = serialize(self)\n",
    "                weights = self.get_weights()\n",
    "                return (unpack, (model, training_config, weights))\n",
    "            cls = Model\n",
    "            cls.__reduce__ = __reduce__\n",
    "        # Run the function\n",
    "        make_keras_picklable()\n",
    "\n",
    "        self._stage_1_model_build()\n",
    "        self._stage_2_model_build()\n",
    "        \n",
    "        self.SKF_list = None\n",
    "\n",
    "        \n",
    "    # Model training\n",
    "    def fit(self, stage_1_model_train_epochs=1):\n",
    "        \n",
    "        '''\n",
    "        Docstring\n",
    "        '''\n",
    "        train_file_list = shuffle(self.train_file_list, random_state=self.random_state)\n",
    "        if self.separate_training_data:\n",
    "            stage_1_model_train_file_list = train_file_list[:len(train_file_list) // 2]\n",
    "            stage_2_model_train_file_list = train_file_list[len(train_file_list) // 2:]\n",
    "        else:\n",
    "            stage_1_model_train_file_list = train_file_list\n",
    "            stage_2_model_train_file_list = train_file_list           \n",
    "        \n",
    "        self._stage_1_model_fit(epochs=stage_1_model_train_epochs, train_file_list=stage_1_model_train_file_list)\n",
    "        self._stage_2_model_fit(class_weight=self.stage_2_model_class_weight, train_file_list=stage_2_model_train_file_list)\n",
    "        \n",
    "        self.random_state += 7\n",
    "        self.stage_1_model_random_state += 7\n",
    "        self.stage_2_model_random_state += 7\n",
    "    \n",
    "        \n",
    "    def predict(self,\n",
    "                input_data=None,\n",
    "                input_type='file_list',         # 'file_list', 'df', 'json'\n",
    "                output_type='df',        # 'df', 'json', 'csv'\n",
    "                output_file_name=None,   # название файла в случае если output_type=='csv'\n",
    "                prod_mode=True,\n",
    "                features=False):\n",
    "        \n",
    "        '''\n",
    "        Docstring\n",
    "        '''\n",
    "        # Data Generator preparation\n",
    "        if input_type == 'file_list':\n",
    "            test_gen = BatchGenerator(file_list=input_data, data_dict=self.data_dict).gen(train_mode=False)\n",
    "        if input_type == 'df':\n",
    "            test_gen = BatchGenerator(df=input_data, data_dict=self.data_dict).gen(train_mode=False)\n",
    "        if input_type == 'json':\n",
    "            test_gen = BatchGenerator(df=pd.DataFrame(json.loads(input_data)['data']), data_dict=self.data_dict).gen(train_mode=False)\n",
    "\n",
    "        # Stage 1 model prediction\n",
    "        test_data = self.stage_1_model.predict(test_gen)\n",
    "        df_test = pd.DataFrame(data=test_data, columns=self.stage_2_model['model_columns'])\n",
    "        del(test_gen, test_data)   \n",
    "  \n",
    "        \n",
    "        # Stage 2 model prediction\n",
    "        df_test['proba'] = 0\n",
    "        for fold in self.stage_2_model['folds']:\n",
    "            df_test['proba'] += fold['trained_model'].predict_proba(df_test)[:,1] / len(self.stage_2_model['folds'])\n",
    "        df_test['score'] = ((1 - df_test['proba']) * 1000).astype(int)\n",
    "        \n",
    "        # forming output columns list\n",
    "        result_cols = ['score']\n",
    "        \n",
    "        if input_type == 'json':\n",
    "            df_test['dataItemId'] = pd.DataFrame(json.loads(input_data)['data'])['dataItemId'].values\n",
    "            result_cols += ['dataItemId']\n",
    "        \n",
    "        if not prod_mode:\n",
    "            result_cols += ['proba']\n",
    "        \n",
    "        if features:\n",
    "            result_cols += self.stage_2_model['model_columns']\n",
    "        \n",
    "        # result output\n",
    "        df_test = df_test[result_cols]\n",
    "        \n",
    "        if output_type == 'df':\n",
    "            return df_test\n",
    "        \n",
    "        if output_type == 'csv':\n",
    "            df_test.to_csv(output_file_name)\n",
    "            return ''\n",
    "            \n",
    "        if output_type == 'json':\n",
    "            # converting numpy data types to JSON compatible data types function\n",
    "            def convert_to_json_type(x):\n",
    "                if str(type(x))[:-4] == \"<class 'numpy.int\":\n",
    "                    x = int(x)\n",
    "                elif str(type(x))[:-4] == \"<class 'numpy.float'>\":\n",
    "                    x = float(x)\n",
    "                else:\n",
    "                    x = str(x) \n",
    "                return x\n",
    "\n",
    "            pred_dict = {'version': self.version, \n",
    "                         'data': [{col: convert_to_json_type(df_test[col].values[i]) for col in df_test.columns} for i in range(df_test.shape[0])]}\n",
    "\n",
    "            return json.dumps(pred_dict)\n",
    "            \n",
    "\n",
    "    # Building Stage 1 model (Bi Directional LSTM model)\n",
    "    def _stage_1_model_build(self):\n",
    "        \n",
    "        data_dict = self.data_dict\n",
    "        optimizer = self.stage_1_model_optimizer\n",
    "        lstm_units = self.stage_1_model_lstm_units\n",
    "        final_dense_units = self.stage_1_model_final_dense_units\n",
    "        random_state = self.stage_1_model_random_state\n",
    "        \n",
    "        np.random.seed(random_state)\n",
    "        set_seed(random_state)\n",
    "\n",
    "        if not optimizer:\n",
    "            optimizer = Nadam(lr=1e-3)    \n",
    "\n",
    "        inputs = []\n",
    "        seq_layers = []\n",
    "        flat_layers = []\n",
    "        \n",
    "        emb_pat = Embedding(input_dim=[col['DICT_SIZE'] for col in data_dict['SEQ']['SEQ']['CAT'] if col['NAME']=='WORST_PAYMT_PAT'][0] + 1,\n",
    "                           output_dim=[col['PROJECTION'] for col in data_dict['SEQ']['SEQ']['CAT'] if col['NAME']=='WORST_PAYMT_PAT'][0],\n",
    "                           trainable=True,\n",
    "                           mask_zero=False,\n",
    "                           name=f\"EMB_PAYMT_PAT\")\n",
    "\n",
    "        emb_mc = Embedding(input_dim=[col['DICT_SIZE'] for col in data_dict['CAT'] if col['NAME']=='MEMBER_CODE'][0] + 1,\n",
    "                           output_dim=[col['PROJECTION'] for col in data_dict['CAT'] if col['NAME']=='MEMBER_CODE'][0],\n",
    "                           trainable=True,\n",
    "                           mask_zero=False,\n",
    "                           name=f\"EMB_MEMBER_CODE\")\n",
    "        \n",
    "        emb_reg = Embedding(input_dim=[col['DICT_SIZE'] for col in data_dict['CAT'] if col['NAME']=='REGION_PR'][0] + 1,\n",
    "                            output_dim=[col['PROJECTION'] for col in data_dict['CAT'] if col['NAME']=='REGION_PR'][0],\n",
    "                            trainable=True,\n",
    "                            mask_zero=False,\n",
    "                            name=f\"EMB_REGION\")\n",
    "\n",
    "        # Sequence RNN layers\n",
    "        for i, seq in enumerate(data_dict['SEQ_ORDER']):\n",
    "            seq_layers.append([])\n",
    "\n",
    "            # Sequence Categorical features layers\n",
    "            for col in data_dict['SEQ'][seq]['CAT']:\n",
    "                inputs.append(Input(shape=(None, ), dtype='uint32', name=f\"{seq}_{col['NAME']}\"))\n",
    "                \n",
    "                emb = Embedding(input_dim=col['DICT_SIZE'] + 1,\n",
    "                                output_dim=col['PROJECTION'],\n",
    "                                trainable=True,\n",
    "                                mask_zero=False,\n",
    "                                name=f\"{seq}_EMB_{col['NAME']}\")(inputs[-1])\n",
    "                \n",
    "                if col['NAME'] == 'MEMBER_CODE':\n",
    "                    emb = emb_mc(inputs[-1])\n",
    "                if col['NAME'] in ['WORST_PAYMT_PAT', 'END_PAYMT_PAT']:\n",
    "                    emb = emb_pat(inputs[-1])\n",
    "\n",
    "                seq_layers[i].append(LayerNormalization()(emb))\n",
    "\n",
    "\n",
    "            # Sequence Numeric features layer\n",
    "            for col in data_dict['SEQ'][seq]['NUM']:\n",
    "                inputs.append(Input(shape=(None, 1,), dtype='float32', name=f\"{seq}_{col}\"))\n",
    "            concat_layer = concatenate(inputs[-len(data_dict['SEQ'][seq]['NUM']):], name=f'{seq}_NUM_CONCAT', axis=2)            \n",
    "            num_norm_layer = BatchNormalization()(concat_layer)\n",
    "            seq_layers[i].append(num_norm_layer)\n",
    "\n",
    "            # All sequence layers concatinating to generate input layer to RNN\n",
    "            concat_layer = concatenate(seq_layers[i], name=f'{seq}_CONCAT', axis=2)\n",
    "\n",
    "            # RNN layers\n",
    "            lstm_stage_1 = LSTM(units=lstm_units, go_backwards=False, name=f'{seq}_LSTM_1', return_sequences=True, return_state=True)(concat_layer)\n",
    "            lstm_stage_2 = LSTM(units=lstm_units, go_backwards=False, name=f'{seq}_LSTM_2', return_sequences=True, return_state=True)(lstm_stage_1[0])\n",
    "            lstm_stage_3 = LSTM(units=lstm_units // 2, go_backwards=False, name=f'{seq}_LSTM_3')(lstm_stage_2[0])\n",
    "            \n",
    "            flat_layers.append(lstm_stage_1[1])\n",
    "            flat_layers.append(lstm_stage_2[1])\n",
    "            flat_layers.append(lstm_stage_3)       \n",
    "\n",
    "        # Sequence Categorical features layers\n",
    "        for col in data_dict['CAT']:\n",
    "            inputs.append(Input(shape=(), dtype='uint32', name=col['NAME']))\n",
    "            emb = Embedding(input_dim=col['DICT_SIZE'] + 1,\n",
    "                                            output_dim=col['PROJECTION'],\n",
    "                                            trainable=True,\n",
    "                                            mask_zero=False,\n",
    "                                            name=f\"EMB_{col['NAME']}\")(inputs[-1])\n",
    "            if col['NAME'] == 'MEMBER_CODE':            \n",
    "                emb = emb_mc(inputs[-1])\n",
    "            if col['NAME'] in ['REGION_PR', 'REGION_REG']:            \n",
    "                emb = emb_reg(inputs[-1])\n",
    "\n",
    "\n",
    "            reshape = Reshape((col['PROJECTION'], ))(emb)\n",
    "            flat_layers.append(LayerNormalization()(reshape))\n",
    "\n",
    "        # Sequence Numeric features layer\n",
    "        for col in data_dict['NUM']:\n",
    "            inputs.append(Input(shape=(len(data_dict['NUM'])), dtype='float32', name=col))\n",
    "            num_norm_layer = BatchNormalization()(inputs[-1])\n",
    "            flat_layers.append(num_norm_layer)\n",
    "\n",
    "        # Concatinating of all sequence layers to generate input layer to RNN\n",
    "        final_concat = concatenate(flat_layers, name=f'FINAL_CONCAT', axis=1)\n",
    "        final_dense = Dense(final_dense_units, activation='relu', name='DENSE')(final_concat)\n",
    "        proba = Dense(2, activation='softmax', name='PROBA')(final_dense)\n",
    "\n",
    "        stage_1_full_model = Model(inputs=inputs, outputs=proba, name='stage_1_full_model')\n",
    "        stage_1_full_model.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "\n",
    "        stage_1_model = Model(inputs=inputs, outputs=final_concat, name='stage_1_model')\n",
    "        stage_1_model.compile(loss='MSE', optimizer=optimizer)\n",
    "        \n",
    "        self.stage_1_full_model = stage_1_full_model        \n",
    "        self.stage_1_model = stage_1_model\n",
    "        \n",
    "    \n",
    "    # Building Stage 2 model (CatBoost classifier)\n",
    "    def _stage_2_model_build(self):\n",
    "        \n",
    "        stage_2_model = catb.CatBoostClassifier(eval_metric=self.stage_2_model_eval_metric,\n",
    "                                                silent=True,\n",
    "                                                iterations=self.stage_2_model_iterations,\n",
    "                                                random_state=self.stage_2_model_random_state)\n",
    "        \n",
    "        self.stage_2_model = {'model_name':'stage_2_model',\n",
    "                              'model_sample':copy.deepcopy(stage_2_model),\n",
    "                              'model_columns':None,\n",
    "                              'feature_importance':None,\n",
    "                              'folds':[{'fold':i,\n",
    "                                        'trained_model':None,\n",
    "                                        'feature_prediction_values_change':None,                                                \n",
    "                                        'feature_loss_function_change':None,\n",
    "                                        'feature_interaction_importance':None} for i in range(self.stage_2_model_SKF_splits)]}\n",
    "        \n",
    "    \n",
    "    # Stage 1 model training\n",
    "    def _stage_1_model_fit(self, train_file_list, epochs=1):\n",
    "            \n",
    "        train_gen = BatchGenerator(file_list=train_file_list, data_dict=self.data_dict, random_state=self.stage_1_model_random_state)\n",
    "        \n",
    "        random_state = self.stage_1_model_random_state\n",
    "        class_weight = self.stage_1_model_class_weight\n",
    "        \n",
    "        np.random.seed(random_state)\n",
    "        set_seed(random_state)\n",
    "\n",
    "        for i in range(epochs):\n",
    "            # Data generator preparation\n",
    "            print(now_str(), f'Stage 1 model training epoch {i+1} / {epochs}')\n",
    "            epoch_train_gen = train_gen.gen(train_mode=True)\n",
    "            \n",
    "            self.stage_1_full_model.fit(epoch_train_gen,\n",
    "                                        steps_per_epoch=train_gen.steps,\n",
    "                                        epochs=1,\n",
    "                                        verbose=True,\n",
    "                                        class_weight=class_weight,\n",
    "                                        shuffle=False)\n",
    "            del(epoch_train_gen)\n",
    "            \n",
    "            #  OOT validation report                   \n",
    "            print(f'{now_str()} Epoch {i + 1} / {epochs} validation\\n')\n",
    "            valid_gen = BatchGenerator(file_list=self.valid_file_list, data_dict=self.data_dict).gen(train_mode=False)\n",
    "            pred = self.stage_1_full_model.predict(valid_gen)\n",
    "            del(valid_gen)\n",
    "            \n",
    "            df = pd.concat([pd.read_parquet(file_name, columns=['TARGET_NAME', self.TARGET_NAME])for file_name in self.valid_file_list], axis=0).reset_index(drop=True)\n",
    "            df['pred'] = pred[:, 1]\n",
    "            del(pred)\n",
    "            short_model_score_report(y_true=df[self.TARGET_NAME],\n",
    "                                     y_pred_proba=df['pred'],\n",
    "                                     name= 'Stage 1 Valid OOT',\n",
    "                                     header=True,\n",
    "                                     model_type='classification')             \n",
    "            \n",
    "            for target in df['TARGET_NAME'].unique():\n",
    "                segment = (df['TARGET_NAME'] == target)\n",
    "                short_model_score_report(y_true=df[segment][self.TARGET_NAME],\n",
    "                                         y_pred_proba=df[segment]['pred'],\n",
    "                                         name= f'Stage 1 Valid OOT {target}',\n",
    "                                         header=False,\n",
    "                                         model_type='classification')\n",
    "            del(df)\n",
    "            print()\n",
    "        # transfer weights to stage_1_model\n",
    "        for i in range(len(self.stage_1_model.layers)):\n",
    "            if self.stage_1_full_model.layers[i].name == self.stage_1_model.layers[i].name:\n",
    "                self.stage_1_model.layers[i].set_weights(self.stage_1_full_model.layers[i].get_weights())\n",
    "    \n",
    "    # Stage 2 model training\n",
    "    def _stage_2_model_fit(self, class_weight, train_file_list):\n",
    "        \n",
    "        # in case if method is called directly\n",
    "        self.stage_2_model_class_weight = class_weight\n",
    "        \n",
    "        TARGET_NAME = self.TARGET_NAME\n",
    "        \n",
    "        print(now_str(), 'Stage 2 training data preparation...')\n",
    "        stage_2_train_data_gen = BatchGenerator(file_list=train_file_list, data_dict=self.data_dict).gen(train_mode=False)\n",
    "        stage_2_train_data = self.stage_1_model.predict(stage_2_train_data_gen)\n",
    "        df = pd.DataFrame(data=stage_2_train_data, columns=[f'F{i:0>3}' for i in range(stage_2_train_data.shape[1])])\n",
    "        self.stage_2_model['model_columns'] = list(df.columns)\n",
    "        df['TARGET_NAME'] = pd.concat([pd.read_parquet(file_name, columns=['TARGET_NAME'])['TARGET_NAME'] for file_name in train_file_list], axis=0).reset_index(drop=True)\n",
    "        df[TARGET_NAME] = pd.concat([pd.read_parquet(file_name, columns=[TARGET_NAME])[TARGET_NAME].astype(int) for file_name in train_file_list], axis=0).reset_index(drop=True)\n",
    "        df['weight'] = df[TARGET_NAME].map(class_weight)\n",
    "        \n",
    "        del(stage_2_train_data, stage_2_train_data_gen)\n",
    "        \n",
    "        print(now_str(), 'Stage 2 OOT data preparation...')\n",
    "        stage_2_oot_data_gen = BatchGenerator(file_list=self.valid_file_list, data_dict=self.data_dict).gen(train_mode=False)\n",
    "        stage_2_oot_data = self.stage_1_model.predict(stage_2_oot_data_gen)\n",
    "        df_oot = pd.DataFrame(data=stage_2_oot_data, columns=[f'F{i:0>3}' for i in range(stage_2_oot_data.shape[1])])\n",
    "        df_oot['TARGET_NAME'] = pd.concat([pd.read_parquet(file_name, columns=['TARGET_NAME'])['TARGET_NAME'] for file_name in self.valid_file_list], axis=0).reset_index(drop=True)\n",
    "        df_oot[TARGET_NAME] = pd.concat([pd.read_parquet(file_name, columns=[TARGET_NAME])[TARGET_NAME].astype(int) for file_name in self.valid_file_list], axis=0).reset_index(drop=True)\n",
    "        del(stage_2_oot_data, stage_2_oot_data_gen)\n",
    "        \n",
    "        # Forming SKF indexes for ansambling and crossvalidation\n",
    "        self._SKF(df=df, n_splits=self.stage_2_model_SKF_splits)\n",
    "        \n",
    "        print(now_str(), 'Stage 2 model training\\n')\n",
    "        \n",
    "        # Making dataset for train results recording\n",
    "        self.valid_oos_pred = df[['TARGET_NAME', TARGET_NAME]]\n",
    "        self.valid_oos_pred['proba'] = None\n",
    "        self.valid_oos_pred['proba'] = self.valid_oos_pred['proba'].astype(float)\n",
    "        \n",
    "        self.valid_oot_pred = df_oot[['TARGET_NAME', TARGET_NAME]]\n",
    "        \n",
    "        for i, (train_index, valid_index) in enumerate(self.SKF_list):\n",
    "            \n",
    "            # Training pool preparation\n",
    "            X_train = df.loc[train_index].drop(columns=['TARGET_NAME', TARGET_NAME, 'weight'])\n",
    "            y_train = df.loc[train_index][TARGET_NAME]\n",
    "            sample_weight = df.loc[train_index]['weight']\n",
    "            \n",
    "            catb_train_pool = catb.Pool(data=X_train, label=y_train, weight=sample_weight)\n",
    "            \n",
    "            # Validation pool preparation\n",
    "            X_valid_oos = df.loc[valid_index].drop(columns=['TARGET_NAME', TARGET_NAME, 'weight'])\n",
    "            y_valid_oos = df.loc[valid_index][TARGET_NAME]\n",
    "            catb_valid_oos_pool = catb.Pool(data=X_valid_oos, label=y_valid_oos)\n",
    "            \n",
    "            model_to_train = copy.deepcopy(self.stage_2_model['model_sample'])\n",
    "            model_to_train.fit(X=catb_train_pool, eval_set=catb_valid_oos_pool, plot=False, use_best_model=True)\n",
    "\n",
    "            # Saving trained model for each fold\n",
    "            self.stage_2_model['folds'][i]['trained_model'] = copy.deepcopy(model_to_train)\n",
    "            # saving features importance data on validation datasets\n",
    "            self.stage_2_model['folds'][i]['feature_prediction_values_change'] = model_to_train.get_feature_importance(data=catb_valid_oos_pool)\n",
    "            self.stage_2_model['folds'][i]['feature_loss_function_change'] = model_to_train.get_feature_importance(type='LossFunctionChange', data=catb_valid_oos_pool)\n",
    "            self.stage_2_model['folds'][i]['feature_interaction_importance'] = model_to_train.get_feature_importance(type='Interaction', data=catb_valid_oos_pool)\n",
    "\n",
    "            # wrighting model predictions on validation dataset\n",
    "            self.valid_oot_pred[f'model_{i}_proba'] = model_to_train.predict_proba(df_oot.drop(columns=['TARGET_NAME', TARGET_NAME]))[:,1]\n",
    "            \n",
    "            self.valid_oos_pred[f'model_{i}_proba'] = model_to_train.predict_proba(df.drop(columns=['TARGET_NAME', TARGET_NAME, 'weight']))[:,1]\n",
    "            self.valid_oos_pred.loc[valid_index, 'proba'] = self.valid_oos_pred.loc[valid_index, f'model_{i}_proba']\n",
    "            self.valid_oos_pred.loc[valid_index, f'model_{i}_proba'] = None\n",
    "            \n",
    "            #  Making report for OOT data\n",
    "            short_model_score_report(y_true=df_oot[TARGET_NAME],\n",
    "                                     y_pred_proba=self.valid_oot_pred[f'model_{i}_proba'],\n",
    "                                     name= f'FOLD {i} Valid OOT',\n",
    "                                     header=(i == 0),\n",
    "                                     model_type='classification')\n",
    "            \n",
    "            #  Making report for OOS data            \n",
    "            short_model_score_report(y_true=y_valid_oos,\n",
    "                                     y_pred_proba=self.valid_oos_pred.loc[valid_index, 'proba'],\n",
    "                                     name=f'FOLD {i} Valid OOS',\n",
    "                                     header=False,\n",
    "                                     model_type='classification')\n",
    "\n",
    "            #  Making report for train data\n",
    "            short_model_score_report(y_true=y_train,\n",
    "                                     y_pred_proba=self.valid_oos_pred.loc[train_index, f'model_{i}_proba'],\n",
    "                                     name= f'FOLD {i} Train data',\n",
    "                                     header=False,\n",
    "                                     model_type='classification')\n",
    "            \n",
    "            print()\n",
    "            del(X_train, y_train, X_valid_oos, y_valid_oos, catb_train_pool, catb_valid_oos_pool)\n",
    "        \n",
    "        # Making final report (all folds))\n",
    "\n",
    "        # Calculation of final scores and probabilities for all models        \n",
    "        self.valid_oos_pred['train_blend_proba'] = self.valid_oos_pred[[f'model_{i}_proba' for i in range(len(self.SKF_list))]].mean(axis=1)\n",
    "        self.valid_oot_pred['train_blend_proba'] = self.valid_oot_pred[[f'model_{i}_proba' for i in range(len(self.SKF_list))]].mean(axis=1)\n",
    "        \n",
    "        \n",
    "        #  Making final report for OOT data\n",
    "        short_model_score_report(y_true=self.valid_oot_pred[TARGET_NAME],\n",
    "                                 y_pred_proba=self.valid_oot_pred['train_blend_proba'],\n",
    "                                 name= f'FINAL Valid OOT',\n",
    "                                 header=True,\n",
    "                                 model_type='classification')\n",
    "\n",
    "        #  Making final report for OOS data            \n",
    "        short_model_score_report(y_true=self.valid_oos_pred[TARGET_NAME], \n",
    "                                 y_pred_proba=self.valid_oos_pred['proba'],\n",
    "                                 name=f'FINAL Valid OOS',\n",
    "                                 header=False,\n",
    "                                 model_type='classification')\n",
    "\n",
    "        #  Making final report for train data\n",
    "        short_model_score_report(y_true=self.valid_oos_pred[TARGET_NAME], \n",
    "                                 y_pred_proba=self.valid_oos_pred['train_blend_proba'],\n",
    "                                 name= f'FINAL Train data',\n",
    "                                 header=False,\n",
    "                                 model_type='classification')\n",
    "        \n",
    "        for target in df['TARGET_NAME'].unique():\n",
    "            print()\n",
    "            #  Making final report for OOT data\n",
    "            short_model_score_report(y_true=self.valid_oot_pred[self.valid_oot_pred['TARGET_NAME']==target][TARGET_NAME],\n",
    "                                     y_pred_proba=self.valid_oot_pred[self.valid_oot_pred['TARGET_NAME']==target]['train_blend_proba'],\n",
    "                                     name= f'FINAL Valid OOT {target}',\n",
    "                                     header=False,\n",
    "                                     model_type='classification')\n",
    "\n",
    "            #  Making final report for OOS data            \n",
    "            short_model_score_report(y_true=self.valid_oos_pred[self.valid_oos_pred['TARGET_NAME']==target][TARGET_NAME],\n",
    "                                     y_pred_proba=self.valid_oos_pred[self.valid_oos_pred['TARGET_NAME']==target]['proba'],\n",
    "                                     name=f'FINAL Valid OOS {target}',\n",
    "                                     header=False,\n",
    "                                     model_type='classification')\n",
    "\n",
    "            #  Making final report for train data\n",
    "            short_model_score_report(y_true=self.valid_oos_pred[self.valid_oos_pred['TARGET_NAME']==target][TARGET_NAME],\n",
    "                                     y_pred_proba=self.valid_oos_pred[self.valid_oos_pred['TARGET_NAME']==target]['train_blend_proba'],\n",
    "                                     name= f'FINAL Train data {target}',\n",
    "                                     header=False,\n",
    "                                     model_type='classification')\n",
    "        \n",
    "        # Calculation of average feature importance\n",
    "        self.stage_2_model['feature_importance'] = self._catb_feature_importance()\n",
    "        \n",
    "        self.valid_oos_pred = None\n",
    "        self.valid_oot_pred = None\n",
    "        self.SKF_list = None\n",
    "        print()\n",
    "        print(now_str(), 'Model training finished\\n')\n",
    "        \n",
    "    # split data into stratified folds\n",
    "    def _SKF(self, df , n_splits):\n",
    "        TARGET_NAME = self.TARGET_NAME\n",
    "        max_layers_qty = 2\n",
    "        random_state = self.random_state\n",
    "        \n",
    "        if n_splits==1:\n",
    "            return [[np.array(df.index), np.array(df.index)]]\n",
    "\n",
    "        # Determine qty of layers\n",
    "        layers_qty = min(max_layers_qty, np.unique(df[TARGET_NAME]).shape[0])\n",
    "        # calculating split points\n",
    "        split_points = np.linspace(np.min(df[TARGET_NAME]), np.max(df[TARGET_NAME]), layers_qty+1)\n",
    "        #split_points = np.quantile(np.sort(np.unique(df[TARGET_NAME])), np.linspace(0, 1, layers_qty+1))\n",
    "\n",
    "        # forming list with indexes and list with training and validation folds\n",
    "        layers=[]\n",
    "        SKF_list = [[np.array([]),np.array([])] for i in range(n_splits)]\n",
    "\n",
    "        np.random.seed(random_state) \n",
    "\n",
    "        for i in range(len(split_points)-1):\n",
    "            # determining indexes to be incuded in layer\n",
    "            if i == len(split_points)-2:\n",
    "                layer_index = np.array(df.loc[(df[TARGET_NAME]>=split_points[i]) & (df[TARGET_NAME]<=split_points[i+1])].index)\n",
    "            else:\n",
    "                layer_index = np.array(df.loc[(df[TARGET_NAME]>=split_points[i]) & (df[TARGET_NAME]< split_points[i+1])].index)\n",
    "\n",
    "            np.random.shuffle(layer_index)\n",
    "            b_qty = layer_index.shape[0] // n_splits\n",
    "\n",
    "            for j in range(n_splits):\n",
    "                if j == n_splits - 1:              \n",
    "                    SKF_list[j][1] = np.hstack((SKF_list[j][1], layer_index[b_qty * j:]))\n",
    "                    SKF_list[j][0] = np.hstack((SKF_list[j][0], layer_index[0:b_qty * j]))\n",
    "                else:\n",
    "                    SKF_list[j][1] = np.hstack((SKF_list[j][1], layer_index[b_qty * j: b_qty * (j+1)]))\n",
    "                    SKF_list[j][0] = np.hstack((SKF_list[j][0], layer_index[0: b_qty * j], layer_index[b_qty * (j+1):]))\n",
    "        \n",
    "        self.SKF_list = SKF_list  \n",
    "    \n",
    "    \n",
    "    # Feature importance for catboostclassifier models\n",
    "    def _catb_feature_importance(self):\n",
    "        \n",
    "        catb_trained_model = self.stage_2_model\n",
    "        \n",
    "        SKF_number = len(catb_trained_model['folds'])\n",
    "        cols = self.stage_2_model['model_columns']\n",
    "\n",
    "        col_dict = {i: col for i, col in enumerate(cols)}\n",
    "\n",
    "        feat_importance = pd.DataFrame({'PredictionValuesChange':np.zeros(len(cols)),\n",
    "                                        'FeatureLossFunctionChange':np.zeros(len(cols))}, index=cols)\n",
    "\n",
    "        for fold in catb_trained_model['folds']:\n",
    "            feat_importance['PredictionValuesChange'] += pd.DataFrame({'feture_importance': fold['trained_model'].get_feature_importance()}, index=cols)['feture_importance'] / SKF_number\n",
    "            feat_importance['FeatureLossFunctionChange'] += pd.DataFrame({'feture_importance': fold['feature_loss_function_change']}, index=cols)['feture_importance'] / SKF_number\n",
    "    \n",
    "        if catb_trained_model['folds'][0]['trained_model'].get_all_params()['depth'] == 1:\n",
    "            feat_importance['feature_interaction'] = np.zeros(feat_importance.shape[0])\n",
    "            return feat_importance.sort_values(by='PredictionValuesChange', ascending=False)\n",
    "        \n",
    "        # making pare wise feature imortance table\n",
    "        fi_interaction = pd.DataFrame(data=catb_trained_model['folds'][0]['feature_interaction_importance'], columns=['f1', 'f2', 'strength'])\n",
    "        fi_interaction['feature_1'] = fi_interaction['f1'].map(col_dict)\n",
    "        fi_interaction['feature_2'] = fi_interaction['f2'].map(col_dict)\n",
    "        fi_interaction['strength_cum'] = np.cumsum(fi_interaction['strength'])\n",
    "        fi_interaction['n_unique'] = None\n",
    "        fi_interaction['unique_features'] = None\n",
    "        for i in range(fi_interaction.shape[0]):\n",
    "            fi_interaction.loc[i, 'n_unique'] = np.unique(np.vstack((fi_interaction.iloc[0:i]['f1'].values, fi_interaction.iloc[0:i]['f2'].values))).shape[0]\n",
    "            unique_fetures = np.unique(np.vstack((fi_interaction.iloc[0:i]['f1'].values, fi_interaction.iloc[0:i]['f2'].values))).astype('int')\n",
    "            fi_interaction.loc[i, 'unique_features'] = str(list(map(col_dict.get, unique_fetures)))[1:-1]\n",
    "\n",
    "        fi_interaction=fi_interaction[['f1','feature_1', 'f2', 'feature_2', 'strength', 'strength_cum', 'n_unique', 'unique_features']]\n",
    "\n",
    "        feat_importance['feature_interaction'] = np.zeros(feat_importance.shape[0])\n",
    "\n",
    "        for i in feat_importance.index:\n",
    "            feat_importance.loc[i, 'feature_interaction'] = fi_interaction.loc[fi_interaction['feature_1']==i, 'strength'].sum() + fi_interaction.loc[fi_interaction['feature_2']==i, 'strength'].sum()\n",
    "        feat_importance['feature_interaction'] =  feat_importance['feature_interaction']  / 2\n",
    "\n",
    "        return feat_importance.sort_values(by='PredictionValuesChange', ascending=False)\n",
    "\n",
    "    \n",
    "    # Saving model container\n",
    "    def save(self, file_name):\n",
    "\n",
    "        with open(file_name, 'wb') as f:\n",
    "            pickle.dump(self, f)\n",
    "            \n",
    "        print('Model saved to file:', file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c5be671",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c251525",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1acacb53",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
